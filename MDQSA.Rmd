---
title: "DHIS_Review"
date: "`r format(Sys.time(), '%b%d%Y')`"
output: 
  html_document:
    code_folding: hide
    css: custom.css
    df_print: tibble
    fig_caption: yes
    keep_md: yes
    toc: yes
    toc_float: yes
editor_options: 
  chunk_output_type: console
params:
  dhis_instance: Malawi
  data_directory:  datasets
  output_directory: 
  output_format: html_document
  display_reports: FALSE 
  create_new: FALSE 
  cache: FALSE
  echo: FALSE
---

## **`r toupper( params$dhis_instance )` **

This script creates the review documents by first downloading and cleaning some data from DHIS2.  Some of these steps require several minutes to an hour.  Those steps that require more than a few minutes are do

```{r package_list, message=FALSE, include= FALSE, warning= FALSE }

# Clear memory
  objs.not.params = ls()[ !grepl( "params" , ls() ) ]
  rm(  list= objs.not.params )

  
# list of required packages
    package.list = c( 'zoo' , "tidyverse" ,"rlist", "listviewer", "DT", "knitr", "kableExtra", "xts", "leaflet", "geojsonR", "wellknown" , "geojsonio" , "sp" , "rmapshaper" , "rgeos" , "RColorBrewer", "DT", "dygraphs", "httr", "jsonlite", "XML", "assertthat", "lubridate", "anytime" , "scales", "RcppRoll", "zoo", "tsibble", "seasonal", "forecast", "stlplus",  "gridExtra", "futile.logger", "Hmisc", "pander" , "stringi", "rlang" , "htmltools" , "tidyselect" , "broom" , "ggthemes" , "readxl", "knitrProgressBar" , 'osmdata', 'sf' ,  "geojsonsf" , 'tmap', 'formattable'  )

# Function to test if package is installed 
    pkgTest <- function( package.list = package.list ){
        
        missing.packages = setdiff( package.list , rownames(installed.packages())) 
        if ( length( missing.packages ) > 0 ) install.packages( missing.packages ) 
    }

# Test if packages loaded
    pkgTest( package.list )

# load the packages
    lapply( package.list, suppressMessages( require ) , character.only = TRUE)

# To render htmo in word doc: https://bookdown.org/yihui/bookdown/html-widgets.html
    # install.packages("webshot")
    # webshot::install_phantomjs()

```


```{r accessory_functions}
    
# load accessory functions
source('dhis2_functions.R')
```

```{r knitr_setup, include = FALSE }

  knitr::opts_chunk$set( echo = params$echo ,
                       comment = "" ,
                       warning = FALSE, 
                       cache = params$cache , 
                       fig.width =  8 ,
                       fig.height = 8
                       )


    # Country/Instance folder
    dhis_instance = params$dhis_instance
    
    origin.folder = paste0( dhis_instance , "/")
     
    if (!dir.exists( origin.folder ) ) dir.create( origin.folder )
    
    
    # sub-folder for data
    dataset.directory = paste0( origin.folder , params$data_directory , "/" )
    
    if (!dir.exists( dataset.directory ) ) dir.create(  dataset.directory )
  
    
    # Type of output:  html, pdf, doc..
    output_format = params$output_format
    
    
    # this folder will hold output of the metadata and data reviews
    output.directory =  ifelse( is.null( params$output_directory ) , 
                                origin.folder , # the directory where this file sits. 
                                paste0(  params$output_directory , "/" , origin.folder )
    )
    
   
    
    
    outputs = tibble( 
        output_format = c(  "html_document", 'word_document'),
        output_suffix = c(  "html", 'doc' )
    )
    
    output.suffix = outputs %>% 
        filter( output_format == params$output_format ) %>% 
        .$output_suffix
    
    
 

   
    # list for passing to parameters to sub-documents
    params.list = list(
        dhis_instance = params$dhis_instance , # selects dhis2 instance
        data_directory = params$data_directory ,  # root data directory
        output_directory = params$output_directory ,
        output_format = params$output_format ,
        cache = FALSE ,
        echo = FALSE 
    )

    
  

    
```


```{r login }

 # login shortcuts

origin.login = function(){
    source( 
    paste0( origin.folder, 
            tolower( params$dhis_instance) , 
            "_login" )
    )

  loginDHIS2( baseurl, username, password)
}

# test login 
 assert_that( origin.login() )
  
```

## Base map of number of org units by admin level

TODO: convert this to a function

```{r OSM_base_map, eval=FALSE }


base_map_file =  paste0( dhis_instance, "_base_map.rda") 

if ( !file.exists( base_map_file ) ){  # stop 
  
  my_world_map <- map_data('world')
  
  # Border countries
  # my_world_map <- my_world_map[my_world_map$region %in% c("Malawi","Zimbabwe", "Mozambique", "Zambia", "Tanzania"),]
  
  # Target country
  my_country_map <- my_world_map[my_world_map$region %in% 
                                   params$dhis_instance, ]
  
  projcrs <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
  
  # convert data.frame to sf points, then to polygon
  my_country_map.sf = st_as_sf( my_country_map   ,                       
             coords = c("long", "lat"),
             crs = projcrs) %>% 
    group_by( group ) %>%
    summarise( do_union=FALSE ) %>%
    st_cast("POLYGON"  ) %>%
    ungroup()
  
  
  # bounding box 
  bbox <- st_bbox( my_country_map.sf) 
  
  # Add water features
  osm_lakes.sf <-
       opq( bbox = bbox  ) %>%
       add_osm_feature(key = 'water', value = 'lake') %>%
       osmdata_sf()  %>%
    .$osm_multipolygons 
  
  
    # simplify to make faster
    osm_lakes.sf =   rmapshaper::ms_simplify(
      input = as( osm_lakes.sf , 'Spatial')
      ) %>%
      st_as_sf()
  
  osm_rivers.sf <-
       opq( bbox = bbox ) %>%
       add_osm_feature(key = 'waterway', value = 'river') %>%
       osmdata_sf()
  
  osm_rivers.sf <- osm_rivers.sf$osm_lines
  
  
  # base 
  
  ### National Park 
  osm_parks.sf <- 
       opq(bbox = bbox ) %>%
       add_osm_feature(key = 'natural', value = 'trunk') %>%
       osmdata_sf()  %>% .$osm_lines 
  
  
  ## add Roads  ####
  
  osm_roads_primary.sf <- 
       opq(bbox = bbox ) %>%
       add_osm_feature(key = 'highway', value = 'trunk') %>%
       osmdata_sf()   %>% 
    .$osm_lines 
  
  osm_roads_secondary.sf <-
       opq(bbox = bbox ) %>%
       add_osm_feature(key = 'highway', value = 'secondary') %>%
       osmdata_sf()  %>% .$osm_lines
  
  # osm_roads_tertiary.sf <- 
  #      opq(bbox =  bbox ) %>%
  #      add_osm_feature(key = 'highway', value = 'tertiary') %>%
  #      osmdata_sf()  %>% .$osm_lines 
  
  
  # Base map ggplot-SF
  base = ggplot() +
       # geom_raster(data = hill.df, aes(lon, lat, fill = hill), alpha = .45) +
       # scale_fill_gradientn(colours = grey.colors(100)) +
       geom_sf(data = osm_lakes.sf , fill = '#9ecae1', colour = NA) +
       # geom_sf(data = osm_rivers.sf, colour = '#9ecae1', size = 0.05) +
       # geom_sf(data = osm_roads_primary.sf, colour = 'red', size = 0.1, alpha = .1 ) +
       # geom_sf(data = osm_roads_secondary.sf, colour = 'brown', size = 0.05, alpha = .1) +
       # geom_sf(data = osm_roads_tertiary.sf, colour = '#636363', size = 0.02) +
       # geom_sf(data = my_bbox_buff_2500.sf, fill = NA) +
       geom_sf(data = my_country_map.sf, fill = NA,
               colour = 'black') +
       coord_sf( xlim = c( bbox['xmin'], bbox['xmax']),
                ylim = c(bbox['ymin'], bbox['ymax'])) +
       theme_bw()
  
  # base
  
  save( base, file = base_map_file )
  
} else {
    
  load(file = base_map_file )
  }

```


## Metadata

### 1. Download DHIS2 Metadata (~5-10 min)

```{r all_meta.  }


  meta_data_file = paste0( dataset.directory , dhis_instance, "_metadata.rds" ) 


  if ( ( !file.exists( meta_data_file) ) ){
   
    
    # origin.login in to server
    origin.login()

    # there are a couple forms of metadata in the api.  This code tests the format, then gets metadata
    # if available, use resources method
    url = paste0( baseurl, "api" )
    resources =  try( get( url )[['resources']] )
    
    # if resources available, get each one      
    if ( !class( resources ) %in% "try-error" ){   
        # View(resources)
        
        n_resources = nrow( resources )
        pb <- progress_estimated( n_resources )
     
        # test:  t = get_resources( 55 )[[1]]
    
        md = map( 1:n_resources , ~get_resources( .x )[[1]] )
        
        null_meta = map_lgl(  1:n_resources ,  ~is.null( md[[.x]]) ) %>% which
        
        # if null, try again
        if ( sum( null_meta ) > 0 ){ 
            
            md.try.again = map( null_meta , ~get_resources( .x )[[1]]  ) 
            names( md.try.again ) = map_chr( null_meta , ~resources$plural[.x] )
            # View( md.try.again )
            
            # if retry worked, update me 
            
            try.again.worked = map_lgl(  1:length(md.try.again) , ~!is.null( md.try.again[[.x]] ) ) %>% which
            
            if ( sum( try.again.worked ) > 0 ){
                
                for ( i in 1:length(try.again.worked ) ){
                    
                    md[[ null_meta[ try.again.worked[i] ] ]] = md.try.again[[ try.again.worked[i] ]]
                }
            }
            
        }
        
        names( md ) = map_chr( 1:n_resources , ~resources$plural[.x] )
        

    } else { # or else, get metadata as one download
        
        url <- paste0( baseurl, "api/metadata.json" )
        
        md =  get( url ) 
    
    }
    
    
    # re-order alaphabetically to be easier to browse in View
    md = md[ order( names(md) ) ]

 
    
    if ( file.exists( meta_data_file) | params$create_new  ){ 
        
        file.rename( meta_data_file ,
                     paste0( meta_data_file , 
                             format(Sys.time(), "%b%d%Y")
                                                        )
                     )
    }
         
    if ( !file.exists( meta_data_file) ) write_rds( md, meta_data_file )
    
  } else {
      
       md = read_rds( meta_data_file )
  }
  
  # Get installed apps
  apps_data_file = paste0( dataset.directory ,
                           dhis_instance, 
                           "_apps.rds" ) 
  
   if ( !file.exists( apps_data_file) | params$create_new ){
   
        origin.login()
    
        # url<-paste0(baseurl,"api/appStore") # available apps ins store
        url<-paste0(baseurl,"api/apps") # installed apps
          
        apps = get( url ) 
          
        write_rds( apps, apps_data_file )
    
  } else {
      
       apps = read_rds( apps_data_file )
  }
  
  # Retrieve access data 
  date_metadata = file.info( meta_data_file )$ctime 
  
```

### 2. Test facility geocodes to see if they are in their respective administrative areas (~2-5 min)

<!-- --> Deprecated  -->

```{r admins. , eval = FALSE }

# if no closdeDate, then set to NA
if ( ! 'closedDate' %in% names( md$organisationUnits ) ){
    
    md$organisationUnits$closedDate = NA
}

# orgUnits that are polygons (likely admin), and open

admins = md$organisationUnits %>%
    select( id, name, coordinates, openingDate, closedDate, path  ) %>%
    as_tibble %>%
    mutate(
        feature_type = map_chr( coordinates, ~feature_type(.x) )
        ) %>%
    filter( feature_type %in% 'Polygon'  ) %>%
    # filter( !is.na(coordinates), is.na( closedDate ) )  %>%
    mutate(

        level = map_int( path,
                         ~length( gregexpr("/", .x, perl = TRUE)[[1]])
            ) ,

        Year = year( ymd_hms( openingDate ) )  %>% ifelse( .<2008 , 2014 , . ) ,

        coordinates = fix_coordinate_brackets( coordinates ) ,

        str_js = paste(
            '{ "type": "MultiPolygon", "coordinates": ' ,
            coordinates , ' }'
            )

    ) %>%
    as_tibble()

## convert geo_json into SF and add other columns (sometimes throws warnings)

  admins.sf = geojson_sf( admins$str_js  ) %>% bind_cols(
    admins %>% select( -str_js, -coordinates )
    )


    char_js. = map( admins$str_js, ~ FROM_GeoJson( .x ) ) 
 
    polys = map( 1:length(char_js.) , 
                 ~map( .x, 
                      ~map( char_js.[[.x]]$coordinates , Polygon ) 
                 )[[1]]
    )
    
    pgons = map( 1:length(polys),  
                 ~ Polygons( polys[[.x]]  , as.character(.x)  ) )
    
    spoly = SpatialPolygons( pgons )
    proj4string( spoly ) = CRS("+proj=longlat +datum=WGS84")

    admins$polygons = spoly
    
    # add centroids
    admins$centroids = geosphere::centroid( admins$polygons )
    admins$long = admins$centroids[ , 1 ]
    admins$lat = admins$centroids[ , 2 ]
    
    # plot( admins$polygons )
    

# NB TODO: Give message for coordinates missing lat long
    
 # save 
 admins_file =  paste0( dataset.directory , dhis_instance, "_admin_boundaries.rds" ) 
 
 if ( file.exists( admins_file ) & params$create_new ) file.rename( admins_file , 
                                                                     paste0( admins_file , 
                                                              format(Sys.time(), "%b%d%Y")
                                                              )
                                            )
 
 saveRDS( admins , admins_file )
 
```

### Points and Polygons as SF

```{r}
ous = ous_from_metatdata( md , simplify = TRUE , SF = TRUE ) 

 # save 
  ous_file =  paste0( dataset.directory , dhis_instance, "_ous.rds" ) 
  
  if ( file.exists( ous_file ) | params$create_new )  file.rename( ous_file , 
                                                                   paste0( ous_file ,
                                                                           format(Sys.time(),
                                                                                  "%b%d%Y") ) 
                                                                   )
 if ( !file.exists( ous_file ) ) saveRDS( ous , ous_file ) 
 
 if ( file.exists( ous_file ) ) ous.sf = readRDS( ous_file )
       
admins = ous %>% filter( feature %in% 'Polygon'  )

hf = ous %>% filter( feature %in% 'Point'  )

# Map of regions 
admins %>%
    filter( level %in% 2 ) %>%
    rename( Region = orgUnit.name ) %>%
    tm_shape( .   ) + 
    tm_polygons( "Region") + 
    # tm_dots( "HF" ) +
    tm_layout( frame = FALSE , legend.outside = TRUE )

# Map of districts 
admins %>%
    filter( level %in% 3 ) %>%
    rename( District = orgUnit.name ) %>%
    tm_shape( . ) + 
    tm_polygons( "District" ) + 
    tm_layout( frame = FALSE , legend.outside = TRUE ) 
    

# map of HF -- not that this all facilities, regardless of whether it supplies OPD data
hf %>%
    # filter( level %in% 3 ) %>%
    mutate( level = factor( level ) ) %>%
    rename( HF = orgUnit.name ) %>%
    tm_shape( . ) + 
    # tm_
    # tm_dots( "level" ) + 
    tm_dots( 'HF' ) + 
    tm_layout( frame = FALSE ) + 
    tm_legend(show=FALSE)
    

# Map of regions with HFC
admins %>%
    filter( level %in% 2 ) %>%
    rename( Region = orgUnit.name ) %>%
    tm_shape( . ) + 
    tm_polygons( "Region" ) + 
    # tm_dots( "HF" ) +
    tm_layout( frame = FALSE  , legend.outside = TRUE ) +
    tm_shape( hf %>% mutate( level = factor( level ) )  ) +  
    tm_dots( "level" , alpha = .2, col = 'black' , size = .01, 
             border.alpha = .2, border.col = 'black' )
```

 <!-- deprecated -->

```{r clinics. , eval=FALSE }

# orgUnits that are points (likely HF), and open
clinics = md$organisationUnits %>% 
    select( id, name, coordinates, openingDate, closedDate, path  ) %>%
    as_tibble %>% 
    mutate( 
        feature_type = map_chr( coordinates, ~feature_type(.x) ) ,
        # remove quotation marks
        coordinates = gsub('"', '', coordinates) ,
        level = map_int( path,
                         ~length( gregexpr("/", .x, perl = TRUE)[[1]]) 
            ) ,
        Year = year( ymd_hms( openingDate ) )  %>% ifelse( .<2014, 2014, .) 
        ) %>%
    filter( !feature_type %in% 'Polygon' ) %>%
    separate( coordinates, c('long', 'lat'), sep = ',', remove = FALSE ) %>% 
    # filter( !is.na(long), !is.na(lat), is.na( closedDate ) ) %>%
    mutate(
        long = as.numeric(  substr(  long, 2, nchar(long) ) ) ,
        lat =  as.numeric(  substr(  lat, 1, nchar(lat)-1 )  ) 
    ) 


 # save 
 facilities_file =  paste0( dataset.directory , dhis_instance, "_facilities.rds" ) 
 
 if ( file.exists( facilities_file ) & params$create_new ){
     
     file.rename( facilities_file , 
                  paste0( facilities_file , 
                          format(Sys.time(), "%b%d%Y")
                          )
                  )
 }
 
 if ( !file.exists( facilities_file ) | params$create_new  ) saveRDS( clinics , facilities_file )
 

```

_ for each non Admin orgUnit, 
- if it is geocoded, check if it is within the admin it belongs to
- if not geocoded, randomly assign geocode with admin it belongs to

Step: find orgUnit with polygon data that it belongs to. Separate path and transform to long lorm

```{r clinics2.}

 # if ( !exists( "clinics" ) | params$create_new ) clinics = readRDS( clinics_file )

# Long form or clinics dataframe with row for each clinic-parent
clinics. = hf %>%
    mutate( 
        paths = purrr::map( path, ~str_split( .x,  "/"  ) ) 
    ) %>% 
    unnest( paths ) %>% unnest( paths ) %>%
    rename( parent.id = paths ) %>%
    group_by( orgUnit ) %>%
    mutate(
        parent.level = row_number() - 1
    ) %>% 
    ungroup() %>% 
  
    filter( parent.level == level - 1 ) %>%
    as_tibble()


```


- Check if facility with coordinates is in parent 

<!-- TODO:  This only works for level one below the admin units.  Does not work for CHW assigned to facilities.  Update code to check if CHW are in the admin area of the facility they are associated with.   -->

```{r facilityInParent. }

 # c is data.frame of clinics with coordinates
 # c.fix, below, is a subset of c, listing clinics whose coordinates are not in a parent
 # c., farther below, is c updated from c.fix and c.impute


 updated_facilities_file =  paste0( dataset.directory , 
                            dhis_instance,
                            "_updated_facilities.rds" 
                            ) 
 

keep_the_existing_one = FALSE # so, dont download a new one.  

  ## This variable overrides the global parameter,  create_new

 if ( ( file.exists( updated_facilities_file ) & !params$create_new ) | keep_the_existing_one ){
     
     c = readRDS( updated_facilities_file ) 
     
     # trim some unwanted cols
     if ( ncol( c ) > 17 )  c = c %>% select( 1:17 )
         
 } else {
     
     nc = nrow(clinics.)
     
     pb <- progress_estimated( nc )
     
     # test if facility is in lowest parent level
     c = clinics. %>% 
         group_by( orgUnit ) %>%
         # filter( parent.level == max( parent.level ) ) %>%
         ungroup() %>%
         mutate(
             in.parent = map2_lgl( orgUnit, parent.id , 
                                   ~is.in.parent(.x, .y, 
                                                 .pb = pb, clinics = clinics. ,
                                                 buffer_arc_seconds = 300 ) 
                                   ) ,
             
             has.coordinates = ifelse( is.na( geometry ) | is.na( geometry ), 
                                       FALSE, TRUE ) ,
             
             has.parent = map_lgl( parent.id , ~.x %in% ous.sf$orgUnit )
         )
 }
 
 
 count(c, level ,  has.coordinates , has.parent,  in.parent  ) 
 
 
```

- Try fixing the ones that are not in parent

```{r c.fix. }
 
 c.to.fix = c %>%
     filter( has.coordinates  %in% TRUE,  has.parent %in% TRUE , in.parent %in% FALSE ) 

 ncfix = nrow(c.to.fix)
 in.parent.fix = vector( "list", length = ncfix ) 
 p <- progress_estimated( ncfix )
 
 for ( i in 1:ncfix ){
     
        p$pause(0)$tick()$print()
     
        in.parent.fix[[i]] = map2_lgl( c.to.fix$orgUnit[i], 
                                   c.to.fix$parent.id[i] , 
                                   ~is.in.parent( .x, .y  , fix = TRUE ,
                                                  .pb = pb, clinics = clinics. )
        )
 }        
 
 c.to.fix$in.parent.fix = in.parent.fix %>% unlist   

 count(c.to.fix, level ,  in.parent.fix )

 # Update c with c.fix
 # c. = left_join( c, c.fix %>% select( id, parent.level, in.parent.fix ), 
 #                by = c( 'id' , 'parent.level' ) 
 #                ) %>%
 #     mutate(
 #         long. = ifelse( in.parent.fix %in% TRUE , lat, long ) ,
 #         lat. = ifelse( in.parent.fix %in% TRUE , long , lat ) ,
 #         long = long. ,
 #         lat = lat.
 #     ) %>%
 #     select( -long., -lat. )
 
 # update rows in ous 
 fixed = c.to.fix$in.parent.fix == TRUE 
 if ( sum( fixed ) > 0 ){
     ids = c.to.fix[ fixed, ]$orgUnit
     fixed.ous.rows = ous$orgUnit %in% ids 
     ous[ fixed.ous.rows , ] = c.to.fix[ fixed , ]
 }
 
```

- If coordinates are missing, or the clinic is not in its parent, then impute a location.

```{r ImputeLocation. }


if ( file.exists( updated_facilities_file ) & !params$create_new )  facilities = readRDS( facilities_file )

if ( !exists( "facilities" ) ){
    
     c.impute = c. %>% 
        filter( 
            !has.coordinates | (!in.parent & !in.parent.fix) , !is.na( parent.name )
            ) %>%
        group_by( id ) %>%
        filter( row_number()==n() )  %>% # select last row
        select( id, parent.id ) %>%
        ungroup
     
     
     ncimpute = nrow( c.impute )
     impute.coords = vector("list", length = ncimpute ) 
     p <- progress_estimated( ncimpute )
     
     for ( i in 1:ncimpute ){
         
            p$pause(0)$tick()$print()
         
            impute.coords[[i]] = impute.location(  c.impute$parent.id[i] ) 
                                    
     }        
     
     c.impute$long.impute  = map_dbl( impute.coords , 1 ) 
     c.impute$lat.impute  = map_dbl( impute.coords , 2 ) 
     
     # glimpse( c.impute )
    
     # Update c. with c.impute
     c.. = left_join( c., c.impute ,  by = c( 'id' , 'parent.id' )  ) %>%
         group_by( id ) %>%
         filter( row_number()==n() )  %>% # select last row
         ungroup %>%
         # use best available coordinates from c.fixed and c.impute 
         mutate( 
             lat. = ifelse( is.na( lat.impute ) , lat, lat.impute ) ,
             long. = ifelse( is.na( long.impute ) , long, long.impute ) ,
             impute = !is.na( long.impute )
             )
     
     if ( file.exists( updated_facilities_file ) & params$create_new ){ 
    
     file.rename( updated_facilities_file , 
                  paste0( updated_facilities_file ,
                          format(Sys.time(), "%b%d%Y")
                          )
                  )
 }
 
    if ( !file.exists( updated_facilities_file ) ) saveRDS( c.. , updated_facilities_file )
 
 }

 
```

### Metadata summary (need to update)`

```{r Metadata_Review., eval = FALSE }
    
  output.filename = paste0( output.directory,
                             dhis_instance, "_Metadata." ,
                             output.suffix
                             ) 

    # rename if exists
    if ( file.exists( output.filename ) & params$create_new ){
        
            file.rename( output.filename , 
                    paste0( output.filename , 
                            paste0( "." ,
                            format( Sys.time(), "%b%d%Y" )
                            )
                            )
                    )
   }

    # create new
    if ( !file.exists( output.filename )  ) { 

        rmarkdown::render( 'Display_Metadata.Rmd' ,
                           
           output_file = output.filename ,
           output_format = output_format ,
           # 'knitrBootstrap::bootstrap_document' ,
           params = params.list ,
           envir = new.env() ,
           encoding = "UTF-8"
           )
         
    }

   if ( file.exists( output.filename ) & params$display_reports ){ 
       
       browseURL( output.filename ) 
   }

```

```{r Display_Brief_Metadata }
    
  output.filename = paste0( output.directory,
                             dhis_instance, "_Brief_Metadata." ,
                             output.suffix
                             ) 

    # rename if exists
    if ( file.exists( output.filename ) & params$create_new ){
        
            file.rename( output.filename , 
                    paste0( output.filename , 
                            paste0( "." ,
                            format( Sys.time(), "%b%d%Y" )
                            )
                            )
                    )
   }

    # create new
    if ( !file.exists( output.filename )  ) { 

        rmarkdown::render( 'Display_Brief_Metadata.Rmd' ,
                           
           output_file = output.filename ,
           output_format = output_format ,
           # 'knitrBootstrap::bootstrap_document' ,
           params = params.list ,
           envir = new.env() ,
           encoding = "UTF-8"
           )
         
    }

   if ( file.exists( output.filename ) & params$display_reports ){ 
       
       browseURL( output.filename ) 
   }

```

```{r levels.}

  # levels
  levels = ou_levels( md )


```

- Create table, 'dsde',  to lookup form associated with eath data element 

```{r dsde. }

dsde = dataSet_dataElement_df( md )

```


## Data elements 


### 3. Query DHIS2 to determine frequency that selected data elements were submitted (~10-90 min)


```{r de.}

    de = md$dataElements
    
    # Convert category combo from df to list (ends up with id as character)
    de[, 'categoryCombo' ] = as.list( de[, 'categoryCombo' ] )
    
    # dataElement vars
    # names( de )
    
    de.grp = md$dataElementGroups

```

```{r search_data_elements. }
    
    mal.grp.de = de.grp[ 
                     grepl( '\\<malaria\\>|palu', 
                            de.grp$name , ignore.case = TRUE) , 
                     "dataElements" 
                     ] %>%
      unlist(.) %>% unname
    

    mal = quo( 
      grepl( '\\<malaria\\>|palu' , de$name , ignore.case = TRUE ) |
        de$name %in% mal.grp.de 
      )
    
    TB = quo( grepl( '\\<tb\\>' , de$name , ignore.case = TRUE ) )
    HIV = quo( grepl( '\\<hiv\\>' , de$name , ignore.case = TRUE ) )
    imm = quo( grepl( 'imm' , de$name , ignore.case = TRUE ) )
    conf = quo( grepl( 'conf' , de$name , ignore.case = TRUE ) )
    susp = quo( grepl( 'susp' , de$name , ignore.case = TRUE ) )
    presumed = quo( grepl( 'sume' , de$name , ignore.case = TRUE ) )
    opd = quo( grepl( '\\<opd\\>' , de$name , ignore.case = TRUE ) )
    ipd = quo( grepl( '\\<ipd\\>' , de$name , ignore.case = TRUE )  )
    patients = quo( grepl( '\\<patient\\>|consult' , de$name , ignore.case = TRUE ) &
         !grepl( '\\<hiv\\>|\\<tb\\>' , de$name , ignore.case = TRUE )
         )
    fever = quo( grepl( '\\<fever\\>' , de$name , ignore.case = TRUE )  &
         !grepl( '\\<yellow\\>|\\<typhoid\\>' , de$name , ignore.case = TRUE )
         )
    attendance = quo( grepl( '\\<attendance\\|consult' , de$name , ignore.case = TRUE ) )
    anc = quo( grepl( '\\<anc\\>|\\<CPN\\>|\\<IPT\\>|\\<TPI\\>' , de$name , ignore.case = TRUE ) )
    
    chw = quo( grepl( '\\<chw\\>|\\<communit\\>|\\<iccm\\>' , de$name , ignore.case = TRUE ) )
    
    deaths = quo( grepl( 'death' , de$name , ignore.case = TRUE ) )
    pop = quo( grepl( 'pop|census|recensement' , de$name , ignore.case = TRUE ) )
    
    stock.grp.de = de.grp[ 
                     grepl( 'stock|rupture', de.grp$name , ignore.case = TRUE) , 
                     "dataElements" 
                     ] %>%
      unlist(.) %>% unname
    
    
    stock = quo( 
      grepl( 'stock|rupture|\\<out\\|\\<sulf\\|\\<arte\\|\\<lufen\\>' ,
             de$name , ignore.case = TRUE ) | de$id %in% stock.grp.de
    )
    
    malaria_stock = quo( 
      ( grepl( 'stock|rupture' , de$name , ignore.case = TRUE ) |
          de$id %in% stock.grp.de )
      &
         grepl( '\\<RDT\\>|\\<rapid\\>|\\<TDR\\>|\\<IPT\\>|\\<ACT\\>|\\<ASAQ\\>|\\<AL\\>|\\<APT\\>|art|lum|pyr|<\\PTI\\>|\\<SP\\>|\\<fansidar\\>|\\<itn\\>|\\<net\\>' , 
                de$name , ignore.case = TRUE ) 
      )
    
# More complicated search for malaria terms
    malaria_terms_whole_words = 'malaria palu PF plasmodium RDT TDR IPT ACT ASAQ AL APT TPI SP fansidar slide micro fever fievre ANC CPN' %>%
        stri_extract_all_words %>% unlist %>%
        
        # add '\\<' so that only whole word is found
        sprintf( "\\<%s\\>", . )
    
    malaria_terms_part_words = NULL  # 'conf susp' %>% stri_extract_all_words %>% unlist
    
    malaria_search_terms = paste(  
        c( malaria_terms_whole_words , 
           malaria_terms_part_words ) ,
        collapse = '|'
    )
    
    malaria_items = quo( grepl( malaria_search_terms, de$name , 
                      ignore.case = TRUE ) 
                      &
         !grepl( '\\<hiv\\>|\\<tb\\>|MTCT|\\<yellow fever\\>|\\<typhoid fever\\>' , de$name , ignore.case = TRUE )
    )
         
# Organise all the searches as a list
    searches = list( mal, malaria_items, patients, fever, attendance, anc, chw, deaths, pop , malaria_stock, TB, HIV, imm, conf, susp, presumed, opd, ipd, stock )
    
    names.searches  = c( 'mal', 'malaria_items',  'patients', 'fever', 'attendance', 'anc', 'chw', 'deaths', 'pop', 'malaria_stock' , 'TB', 'HIV', 'imm', 'conf', 'susp', 'presumed', 'opd', 'ipd',  'stock')
    
    
    names( searches ) = names.searches 
    
```

### Request number of reports during last 12 months for each **data element**

```{r dataElementReporting }

  all.de = Reduce(  "|" , map( searches, ~eval_tidy(.x)  ) )
  # sum( all.de )

  display.vars = c('name', 'zeroIsSignificant', 'lastUpdated',
                                 'categoryCombo', 'id' )
     
  de.mal = de[  eval_tidy( malaria_items) , display.vars ]
  
  de.search = de[ all.de, ]

  reported_data_file = paste0( dataset.directory ,
                           dhis_instance, 
                           "_reported.rds" ) 
  
  
  new.dataElements = NULL

  
  
  if ( file.exists( reported_data_file) | !params$create_new ){
    
     d.reported =  readRDS( reported_data_file )
     
     d.reported.dataElements =  count( d.reported , dataElement ) %>% .$dataElement 
     
  } else {
      
      d.reported.dataElements = NULL 
  }
  
  
  new.dataElements =  setdiff( de.search$id[ order( de.search$id )] ,
                               d.reported.dataElements 
                               )

  nde = length( new.dataElements )
  
  # Get number reported for all elements
  if ( nde > 0 ){
        
        d.reported = api_last12months_national_data( 
            
            de.include = de[ all.de , ] ,
            file = reported_data_file ,
            details = TRUE 
            
            )
        

        saveRDS( d.reported , reported_data_file )
        
  } 

  

```


### Request Data Completeness (form submissions) : number of  **data form** reported during last 12 months ('DHIS2 reporting')

```{r download_dataset_submissions.  }
## DataElements : details 

  dataset.reporting.file = paste0( dataset.directory , 
                     dhis_instance ,  
                     "_dataset_reporting.rds" )

    if ( !file.exists( dataset.reporting.file ) | params$create_new ){
      
    # get datasets associated with data_totals dataElements
    datasets =  dsde  %>% 
       filter( dataElement.id %in% de[ all.de , ]$id ) %>%
       count( dataSet , dataSet.id ) %>% 
       # convert ids to names
       rename( id = dataSet.id ) %>%
       left_join( md$dataSets %>% select( name, id ), 
                  by = "id" 
       ) %>%
       
       # pretend datasets are dataelements
       rename( dataElement.id = id ,  
               dataElement = name
       ) 

            
        dataset_reporting = api_dataset( 
                                levels = levels.vector[1] , 
                                
                                de.dataset =   datasets , 
                    
                                file = dataset.reporting.file 
                                
                                )
        
     } else {
        
        dataset_reporting = readRDS( dataset.reporting.file ) 
        
    }         
          
    
   # Rename existing file
   if ( file.exists( dataset.reporting.file ) & params$create_new ){
        
        file.rename( dataset.reporting.file ,
                     paste( dataset.reporting.file ,
                            format( Sys.time(), "%b%d%Y" )
                            )
                     )
    }
    
    # save new
    if ( !file.exists( dataset.reporting.file) ) saveRDS( dataset_reporting , dataset.reporting.file )

    # get date
    dataset_reporting_date = file.info( dataset.reporting.file )$mtime

```

### Number facilities reporting last year.  

```{r facilities_reporting_last_year }

open_ou =     md$organisationUnits %>% 
    select( id, closedDate) %>%
    filter( is.na(closedDate) ) %>%
    select( id )

n_facilities_assigned_to_datasets = md$dataSets %>%
    select( id, organisationUnits) %>%
    rename( dataSet.id = id ) %>% 
    unnest( organisationUnits ) %>%
    # left_join( open_ou , by = 'id' ) %>%
    count( dataSet.id ) %>%
    rename( n_facilities = n )
    
# 1-year period to use for summary
summary_year = min( d.reported$period )

# mean percent of facilities submitting dataset-report

 percent_dataset_reporting_last_year =  dataset_reporting %>% 
    separate( dataElement, 
              into = c( 'dataSet', 'reporting' ) , 
              sep = "\\."  ) %>%
    filter(  substr( period , 1, 4 ) %in%  summary_year ,
             level == 1 )  %>%
     mutate( value = as.integer(value)) %>%
     spread( reporting, value ) %>%
     group_by( dataSet ) %>%
     summarise( 
         forms_submitted_lastyear = sum( ACTUAL_REPORTS , 
                                     na.rm = TRUE) , 
         forms_expected_lastyear = sum( EXPECTED_REPORTS , 
                                    na.rm = TRUE ) ,
         forms_ontime_lastyear = sum( ACTUAL_REPORTS_ON_TIME , 
                                  na.rm = TRUE ) ,
         form_submission_rate = forms_submitted_lastyear /
           forms_expected_lastyear
         ) 
 
formattable( percent_dataset_reporting_last_year )
```


```{r d.mal.reported. }

# number of options in each category combo

options = md$categoryCombos %>% 
    select( id, categories) %>% 
    unnest( categories ) %>%
    rename( categoryCombo.id = id , category = id1 ) %>%
    inner_join( md$categories  %>% select( id, categoryOptions ), 
                by = c('category' = 'id' )
                ) %>%
    unnest( categoryOptions ) %>% 
    count( categoryCombo ) %>%
    rename( n_options = n )

# Category Option Combos
 categoryOptions = 
  
  md$categoryOptionCombos %>% 
  select( id, name ) %>% 
  rename( 
    categoryOptionCombo = name , 
    categoryOptionCombo.id = id ) %>%
  mutate( categoryCombo.id = 
            md$categoryOptionCombos$categoryCombo$id ) %>%
  inner_join( md$categoryCombos %>% select( id, name ) %>% 
                rename( categoryCombo = name ),
              by = c( 'categoryCombo.id' = 'id' ) )%>% 
  group_by( categoryCombo.id, categoryCombo ) %>%
  summarise( 
    n_categoryOptions = n() , 
    categories = paste( categoryOptionCombo , collapse = '; '  )
  )
 
 # compare options with categoryOptionCombos
# glimpse( options )
# glimpse( categoryOptions )
# left_join( options, categoryOptions , by=c('categoryCombo'='categoryCombo.id') ) %>% View

# Select all malaria relevent variables
malaria.groups = setdiff( names(searches) , c("TB", "imm", "HIV", "opd", "idp", "susp", "conf", "presumed", "stock", "deaths") )
                            
 all.possible.malaria = Reduce(  "|" , map( searches[ malaria.groups ], eval_tidy  ) )
  # sum( all.possible.malaria.de )
  
de.all.possible.malaria = de[ all.possible.malaria , display.vars ] 

# View( de.selected )

d.mal.reported = d.reported[ , c("dataElement", "value")] %>% 
    
    rename( dataElement.id = dataElement ) %>%
    
    inner_join(

        count( dsde, dataElement.id, categoryCombo) %>% select(-n) ,

        by = c("dataElement.id")

        ) %>%
    
    left_join(
        
        categoryOptions ,
        
        by = c( 'categoryCombo' = 'categoryCombo.id' )
        
        ) %>% 
    
   left_join( 
       
       dsde  %>% select( -categoryCombo ), 
       
       by = 'dataElement.id' 
       
       ) %>%  
  
   left_join( md$dataElements %>% 
                select( id, zeroIsSignificant ) %>%
                rename( dataElement.id = id ),
              by = 'dataElement.id' ) %>%
    
   left_join( 
       
       md$dataSets %>% select( id, periodType, timelyDays
                               , organisationUnits
                               ) %>%
           bind_cols( dataEntryForm.id = md$dataSets$dataEntryForm$id  ) ,
       
       by = c( 'dataSet.id' = 'id' )
       
       ) %>% 
    
    left_join( md$dataEntryForms %>% select( id, name ) %>%
                   rename( dataEntryForm = name , dataEntryForm.id = id  ),
               
               by = 'dataEntryForm.id'  ) %>% 
    
    left_join( 
        
        n_facilities_assigned_to_datasets , by = 'dataSet.id'
        
        ) %>% 
    
    mutate( 
        
        val = ifelse( is.na( value ) , NA, as.integer( value ) )

        , frequency = case_when(
            periodType %in% "Weekly" ~ 52 ,
            periodType %in% "Monthly" ~ 12 ,
            periodType  %in% "Quarterly" ~ 4 ,
            periodType  %in% "Yearly" ~ 1 ,
            TRUE ~ 1
        )
        
        
        , expected = frequency * n_facilities * n_categoryOptions
            
        , pVal = ifelse( !is.na(expected) , val / expected , NA)

            
        ) %>% 
    
    mutate( 
                Percent_Reported = sprintf("%1.2f%%", 100 * pVal ) 
                )  

# If a data element linked to >1 dataSet, select the one with greatest number of facilities
    d.mal.reported.top.dataset = d.mal.reported %>%
        group_by( dataElement.id , dataElement ) %>%
        arrange( -n_facilities  ) %>%
        filter(  row_number() == 1 ) %>%
        arrange( desc(pVal) ) 

# Warning: data too big for datatable
#   It seems your data is too big for client-side DataTables. You may consider server-side processing: https://rstudio.github.io/DT/server.html
    
   datatable( d.mal.reported.top.dataset , filter = "top")
   
   formattable( d.mal.reported.top.dataset  )
```


### Create spreadsheet with data elements that can be used to select key variables 

```{r dataElement_spreadsheet}

     de.review.xlsx = paste0( dataset.directory , 
                             dhis_instance , 
                             "_dataElements.xlsx" )

    # previous.include -- if some data already downloaded 
        downloaded = NA 
                    
        data.totals.file = paste0( dataset.directory ,
                           dhis_instance, 
                           "_totals.rds" )

        if ( file.exists( data.totals.file ) ){
            
            data_totals = readRDS( data.totals.file ) 
            
            downloaded = count( data_totals, dataElement ) %>%
                pull( dataElement )
        }

    # if file exists and want to re-write, rename
    if ( file.exists( de.review.xlsx ) & params$create_new ){
        
        
        # rename file
        file.rename( de.review.xlsx , paste0( de.review.xlsx , 
                                            format(Sys.time(),
                                                   "%b%d%Y")
        ))
      
    } 
    
   # if file does not exist, create
    if ( !file.exists( de.review.xlsx ) ){
      
    # Create excel worksheet with separate tab for each search
      
      
    display.vars = c('name', 'zeroIsSignificant', 'lastUpdated',
                                 'categoryCombo', 'id' )


    data_element_info = map( seq_along( searches ) ,
            
            ~ de[ eval_tidy( searches[[.x]] )  , display.vars ] %>%
                
            rename( categoryCombo.id = categoryCombo , 
                    dataElement.id = id,
                    dataElement = name ) %>%
                
            mutate( lastUpdated = format( ymd_hms( lastUpdated ), '%b %Y' ) ) %>%
            
            # categoryCombos
            left_join(  
                categoryOptions , 
                        by = 'categoryCombo.id'
                        )  %>%  
                
            # dataElementGroups
            left_join( 
                md$dataElementGroups %>% 
                    select( name, dataElements) %>%
                    unnest %>% 
                    rename(  dataElement.id = id ) %>%
                    group_by( dataElement.id ) %>%
                    summarise( 
                        Group = paste( name, collapse = " / ")
                        ) ,
                by = 'dataElement.id'
                ) %>%  
                  
            # dataSets  
              
            left_join(
                dsde %>% select( dataElement.id, dataSet.id , dataSet)  ,
                by = c( 'dataElement.id')
                ) %>% 
                
            left_join( 
                
                md$dataSets %>% select( id, periodType, timelyDays
                                , organisationUnits
                                ) %>%
                    
                    bind_cols( dataEntryForm.id = md$dataSets$dataEntryForm$id  ) , 
                
                by = c( 'dataSet.id' = 'id' )
                
                ) %>% 
                
                left_join( md$dataEntryForms %>% 
                                   select( id, name ) %>%
                                   rename( dataEntryForm = name , dataEntryForm.id = id  )             
              , by = 'dataEntryForm.id'  
               
               ) %>% 
    
                left_join( 
                    
                    n_facilities_assigned_to_datasets , by = 'dataSet.id'
                    
                ) %>%     
                
        # number reported       
        left_join( d.mal.reported.top.dataset %>%
                       group_by( dataElement.id ) %>%
                       summarise(
                           value = sum( val, na.rm = TRUE ) ,
                           pVal = max( pVal, na.rm = TRUE )) ,
                   by = c( "dataElement.id" )

                   ) %>%
            
         left_join( percent_dataset_reporting_last_year  , 
                    by = c( 'dataSet.id' = "dataSet")

                    ) %>%
 
            mutate( 
        
        val = ifelse( is.na( value ) , NA, as.integer( value ) )
        
        , frequency = case_when(
            periodType %in% "Weekly" ~ 52 ,
            periodType %in% "Monthly" ~ 12 ,
            periodType  %in% "Quarterly" ~ 4 ,
            periodType  %in% "Yearly" ~ 1 ,
            TRUE ~ 1
        )
        
        ,  Reports_Last_Year = as.integer( val )
        
        , expected = frequency * n_facilities * n_categoryOptions
        
        # , expected = forms_expected_lastyear * n_categoryOptions
        # 
        , pVal = ifelse( !is.na(expected) , val / expected , NA)
        # 
        , Include = if( is.na( downloaded ) ){
              ifelse( pVal>.25, "Y" ,  NA ) 
              
              } else {
                
                ifelse( dataElement.id %in% downloaded ,  
                      "Y" , NA ) 
                }
        
    ) %>% 
    
    mutate( 
        Percent_Reported = sprintf("%1.2f%%", 100 * pVal ) 
    )  %>% 
    
    rename( expected_facilities = n_facilities , pReporting_Data_Element  = pVal ) %>%
    
    select( dataElement.id, dataElement , Include,
            Reports_Last_Year , 
            expected_facilities,  
            Percent_Reported , 
            zeroIsSignificant ,  periodType, dataSet, Group ,
           categoryCombo , categories, timelyDays , 
           dataEntryForm ) %>%
        
    arrange( -Reports_Last_Year,  dataElement )

    )
    
    names( data_element_info ) = names.searches
    
    # file.copy( 'dataElements_template.xlsm', de.review.xlsx )
    
    try( openxlsx::write.xlsx( data_element_info , 
                               de.review.xlsx ) 
         )
    
    
    }
    

```

```{r dataElement_Selection. }

  
    # if exist, open excel file
    if ( file.exists( de.review.xlsx ) 
         # & params$display_reports == TRUE 
         ){
        
        browseURL( de.review.xlsx )
        
    }  
    
    
```

## Selected Key ELements


```{r data_elements_include., eval = FALSE  }
    
    sheets = excel_sheets( de.review.xlsx ) 
    
    use_these_sheets = sheets %in% c( "malaria_stock", "stock", "pop", "attendance",  "malaria_items", "mal" , "deaths" )
    
    target_sheets = sheets[ use_these_sheets ]  
    
    # get selected rows from every spreadsheet tab
    de.include = map_df(  seq_along( target_sheets ) , 
                         ~readxl::read_xlsx( de.review.xlsx ,  sheet = target_sheets[.x] ) %>% 
                             filter( !is.na(Include) ) ) %>%
        
        distinct( dataElement.id, .keep_all = TRUE ) %>%
            
            mutate( previously_downloaded =  dataElement.id %in% downloaded )
            

  rmarkdown::paged_table( de.include )

```

### Selected Elements by Topic

```{r, eval= FALSE}

sheets = excel_sheets( de.review.xlsx ) 

topics = c( '', '', '', '', '',  "Confirmed", 'Suspected', '', '', 'Attendance', 'Fever', 
            'Attendance', 'ANC', 'Deaths', 'Pop' , 'Stock', 'Stock' )

de.include = 
    map_df(  seq_along( sheets ) , 
         ~readxl::read_xlsx( de.review.xlsx ,  sheet = sheets[.x] ) %>%
             mutate( Search = sheets[.x] , 
                     topic = factor( Search, levels = sheets ,
                                     labels = topics ) 
                     ) %>%
           filter( !is.na( Include )  ) %>%
           select( Search, topic , dataElement.id , dataElement ) 
) 

de.include = de.include %>%
    semi_join( de.include , by = c("dataElement.id", "dataElement") ) %>%
    group_by( dataElement.id, dataElement ) %>% 
    summarise( topic = paste0( unique( topic ), collapse = "")) %>% 
    distinct() %>%  
    arrange( topic, dataElement ) 

```

```{r de.include_spreadsheet }

 de.include.xlsx = paste0( dataset.directory , 
                             dhis_instance , 
                             "_dataElements_selected.xlsx" )
 
# if file exists, rename
  if ( file.exists( de.include.xlsx) & params$create_new   ){ 
        
        file.rename( de.include.xlsx ,
                     paste0( de.include.xlsx , 
                             format(Sys.time(), "%b%d%Y")
                                                        )
                     )
     }
 
 
 if ( !file.exists( de.include.xlsx) | params$create_new  ) {
   
   try( openxlsx::write.xlsx( de.include , de.include.xlsx ) )
   
 }

 # open file 
if ( file.exists(  de.include.xlsx  ) ) {
       
       browseURL(  de.include.xlsx  ) 
       
}

```

### Table summary

```{r data_element_table }

# read variables and topics from de.include spreadsheet
de.include = readxl::read_excel( de.include.xlsx  )

# Get data elements meta-details

meta_details = d.mal.reported %>%  
                   ungroup %>% 
                   select( dataElement , categories , dataSet, n_facilities, zeroIsSignificant , periodType, timelyDays )
                
## Combine
data_element_table = de.include %>%
  
    filter( !is.na( Include ) ) %>%
  
    left_join( 
        meta_details  
        , by = "dataElement"
) %>% 

    group_by( dataElement.id ) %>%
    summarise_all( 
        funs( paste( unique(.) , collapse = ';\n' ) ) 
        ) %>%
    
    # mutate( topic = factor( topic , levels = unique( topics ) ) ) %>% 
    
    arrange( 
      # topic, 
      dataElement )

data_element_table %>% rmarkdown::paged_table(.) 


# set de.include  to be the reduced set of variables in data_element_table
de.include = data_element_table 

```


## Request Data Totals 


```{r data_totals_count}

    data.totals.count.file = paste0( dataset.directory , 
                               dhis_instance, "_count_totals.rds" )

    nde = nrow( de.include )
    
    new.variables.request = sum( !de.include$dataElement.id %in% downloaded )  > 0 
    
    new.variables = de.include %>%
      filter( !dataElement.id %in% downloaded )

    # if doesn't exist or creating new, get dataset
    if ( nde > 0 & ( params$create_new | !file.exists( data.totals.count.file ) | new.variables.request ) ){
    
        data_count_totals = api_data( 
            
            levels = levels.vector , 
            de.vars = de.include ,
            folder = dataset.directory ,
            instance = params$dhis_instance ,
            details = FALSE  ,
            aggregationType = 'COUNT' )
        
        print( paste( "TOTAL:", 
                       scales::comma( nrow( data_count_totals ) ) , 
                       "records"  ) )
        
    # rename old one
    
      # Retrieve access data 
      date_file = file.info( data.totals.count.file )$ctime 
        
        file.rename( data.totals.count.file ,
                     paste0( data.totals.count.file , "." ,
                            format( date_file , "%b%d%Y" )
                            )
                     )

    if ( !file.exists( data.totals.count.file) ) saveRDS( data_count_totals , data.totals.count.file )
        
    
    } else {
        
        # load previously downloaded dataset
        data_count_totals = readRDS( data.totals.count.file )
    }
    

    # get date
    data_count_totals_date = file.info( data.totals.count.file )$mtime
        

```


```{r data_totals}

    data.totals.file = paste0( dataset.directory , 
                               dhis_instance, "_totals.rds" )

    nde = nrow( de.include )
    
    new.variables.request = sum( !de.include$dataElement.id %in% downloaded )  > 0 
    
    new.variables = de.include %>%
      filter( !dataElement.id %in% downloaded )

    # if doesn't exist or creating new, get dataset
    if ( nde > 0 & ( params$create_new | !file.exists( data.totals.file ) | new.variables.request ) ){
    
        data_totals = api_data( 
            
            levels = levels.vector , 
            de.vars = de.include ,
            folder = dataset.directory ,
            instance = params$dhis_instance ,
            details = FALSE  )
        
        print( paste( "TOTAL:", 
                       scales::comma( nrow( data_totals ) ) , 
                       "records"  ) )
        
    # rename old one
    
      # Retrieve access data 
      date_file = file.info( data.totals.file )$ctime 
        
        file.rename( data.totals.file ,
                     paste0( data.totals.file , "." ,
                            format( date_file , "%b%d%Y" )
                            )
                     )

    if ( !file.exists( data.totals.file) ) saveRDS( data_totals , data.totals.file )
        
    
    } else {
        
        # load previously downloaded dataset
        data_totals = readRDS( data.totals.file )
    }
    

    # get date
    data_totals_date = file.info( data.totals.file )$mtime
        

```


## Request Data Details 


```{r data_details_count}

    data.details.count.file = paste0( dataset.directory , 
                               dhis_instance,
                               "_count_details.rds" )

    nde = nrow( de.include )
    
    new.variables.request = sum( !de.include$dataElement.id %in% downloaded )  > 0 
    
    new.variables = de.include %>%
      filter( !dataElement.id %in% downloaded )

    # if doesn't exist or creating new, get dataset
    if ( nde > 0 & ( params$create_new | !file.exists( data.details.count.file ) | new.variables.request ) ){
    
        data_count_details = api_data( 
            
            levels = levels.vector , 
            de.vars = de.include ,
            folder = dataset.directory ,
            instance = params$dhis_instance ,
            details = TRUE  ,
            aggregationType = 'COUNT' )
        
        print( paste( "TOTAL:", 
                       scales::comma( nrow( data_count_details ) ) , 
                       "records"  ) )
        
    # rename old one
    
      # Retrieve access data 
      date_file = file.info( data.details.count.file )$ctime 
        
        file.rename( data.details.count.file ,
                     paste0( data.details.count.file , "." ,
                            format( date_file , "%b%d%Y" )
                            )
                     )

    if ( !file.exists( data.details.count.file) ) saveRDS( data_count_details , data.details.count.file )
        
    
    } else {
        
        # load previously downloaded dataset
        data_count_details = readRDS( data.details.count.file )
    }
    

    # get date
    data_count_details_date = file.info( data.details.count.file )$mtime
        

```


```{r data_details}
## DataElements : details 

    data.details.file = paste0( dataset.directory ,
                                dhis_instance, 
                                "_details.rds" )

    if ( nde > 0 & ( params$create_new | !file.exists( data.details.file ) | new.variables.request ) ){
            
        data_details = api_data( 
            
            levels = levels.vector , 
            de.vars = de.include ,
            folder = dataset.directory ,
            instance = params$dhis_instance ,
            details = TRUE
            
            )
        
        print( paste( "TOTAL:", 
                  scales::comma( nrow( data_details ) ) , 
                  "records"  ) )
    
    
     # Retrieve access data 
    date_file = file.info( data.details.file )$ctime 

     if (file.exists( data.details.file )){   
       file.rename( data.details.file ,
                     paste0( data.details.file , "." ,
                            format( date_file , "%b%d%Y" )
                            )
                     )
     }
    

    saveRDS( data_details , data.details.file )
    
    } 

    if ( !exists( "data.details" ) )  data_details = readRDS( data.details.file )   
    
    
    # get date
    data_details_date = file.info( data.details.file )$mtime
        

```



## Request Indicator Details 


```{r indicators}

indicators = md$indicators

malaria_terms = 'malaria palu RDT TDR IPT ACT ASAQ AL APT PTI SP slide fever fiev'

malaria_search_terms = unlist( stri_extract_all_words(malaria_terms) )

# add '\\<' so that only whole word is found
malaria_search_terms_words = paste0( '\\<', malaria_search_terms, '\\>')

mal_indicators = grepl( paste( malaria_search_terms, collapse = '|'), indicators$displayName , ignore.case = TRUE ) |
  grepl( paste( malaria_search_terms, collapse = '|'), indicators$name , ignore.case = TRUE )

mal_indicators_n = sum( mal_indicators )

cols = map_lgl( indicators, ~!is.list(.x) )



datatable( indicators[ mal_indicators , c( 'name', 'displayName' ) ]   )

 datatable( indicators[mal_indicators, cols ] )
 

```

## There are `r nrow(indicators)` indicators.  Of these, `r mal_indicators_n` have a malaria seach term that includes: `r paste( malaria_terms) `.

```{r ind_include}

# for Burkina Faso, 675 is Paludisme confirme

ind.include = indicators[ 675 , cols ]
```

```{r data_indicators_count}

    data.indicators.count.file = paste0( dataset.directory , 
                               dhis_instance,
                               "_count_indicators.rds" )

    nde = nrow( ind.include )
    
    new.variables.request = sum( !ind.include$id %in% downloaded )  > 0 
    
    new.variables = ind.include %>%
      filter( !id %in% downloaded )

    # if doesn't exist or creating new, get dataset
    if ( nde > 0 & ( params$create_new | !file.exists( data.indicators.count.file ) | new.variables.request ) ){
    
        data_count_indicators = api_data( 
            
            levels = levels.vector , 
            de.vars = ind.include ,
            folder = dataset.directory ,
            instance = params$dhis_instance ,
            details = FALSE  ,
            aggregationType = 'COUNT' )
        
        print( paste( "TOTAL:", 
                       scales::comma( nrow( data_count_indicators ) ) , 
                       "records"  ) )
        
    # rename old one
    
      # Retrieve access data 
      date_file = file.info( data.indicators.count.file )$ctime 
        
        file.rename( data.indicators.count.file ,
                     paste0( data.indicators.count.file , "." ,
                            format( date_file , "%b%d%Y" )
                            )
                     )

    if ( !file.exists( data.indicators.count.file) ) saveRDS( data_count_indicators , data.indicators.count.file )
        
    
    } else {
        
        # load previously downloaded dataset
        data_count_indicators = readRDS( data.indicators.count.file )
    }
    

    # get date
    data_count_indicators_date = file.info( data.indicators.count.file )$mtime
        

```


```{r data_indicators}
## DataElements : indicators 

    data.indicators.file = paste0( dataset.directory ,
                                dhis_instance, 
                                "_indicators.rds" )

    if ( nde > 0 & ( params$create_new | !file.exists( data.indicators.file ) | new.variables.request ) ){
            
        data_indicators = api_data( 
            
            levels = levels.vector , 
            de.vars = ind.include ,
            folder = dataset.directory ,
            instance = params$dhis_instance ,
            details = FALSE  ,
            aggregationType = 'SUM' 
            
            )
        
        print( paste( "TOTAL:", 
                  scales::comma( nrow( data_indicators ) ) , 
                  "records"  ) )
    
    
     # Retrieve access data 
    date_file = file.info( data.indicators.file )$ctime 

     if (file.exists( data.indicators.file )){   
       file.rename( data.indicators.file ,
                     paste0( data.indicators.file , "." ,
                            format( date_file , "%b%d%Y" )
                            )
                     )
     }
    

    saveRDS( data_indicators , data.indicators.file )
    
    } 

    if ( !exists( "data.indicators" ) )  data_indicators = readRDS( data.indicators.file )   
    
    
    # get date
    data_indicators_date = file.info( data.indicators.file )$mtime
        

```

# Summarise data

```{r display_data.  }

    output.filename = paste0( origin.folder ,
                              dhis_instance, "_data." ,
                              output.suffix
                             ) 


    if ( file.exists( output.filename ) & params$create_new ){
        
            file.rename( output.filename , 
                    paste0( output.filename ,
                            format( Sys.time(), "%b%d%Y" ) 
                    )
                    )
   }


    if ( !file.exists( output.filename ) ) { 

        rmarkdown::render( 'Display_Data.Rmd' ,
                           
           output_file = paste0( output.filename  ),
           output_format = output_format ,
           # 'knitrBootstrap::bootstrap_document' ,
           params = params.list ,
           envir = new.env() ,
           encoding = "UTF-8"
           )
         
    }


   # open file

   if ( file.exists( output.filename ) &  params$display_reports ) {
       
       browseURL( output.filename ) 
       
   }

```



# Trends

## Quantitative Quality

```{r  updated_ous }

source( "Trend code/Trend_Code.R")


# use updated ous
ous.updated = readRDS( updated_facilities_file ) %>% 
    rename( orgUnit = id, orgUnit.name = name , feature = feature_type ,
            parent_ou = parent.id , parent_ou.name = parent.name
    ) %>%
    mutate( lat = lat. , long = long. )

# add level.name if missing
if ( !"level.name" %in% names( ous.updated ) ){
    
    ous.updated = ous.updated %>%
        left_join( 
            md$organisationUnitLevels[ , c('level', 'name')] %>% 
                rename( level.name = name ) ,
            by = 'level' 
            )
}

# glimpse(ous.updated)
# names(ous.updated)
# imputed.ous = ous.updated %>% filter( impute == TRUE ) %>% .$orgUnit

ous = ous.translated( .meta = md )

# names(ous)

# ous = ous %>% left_join( ous.updated %>% 
#                              select( orgUnit, lat, long, impute ) ,
#                          by = 'orgUnit'
# )

```

### Orgunits with details ('translated')

```{r translate_data_details}

if ( !exists( "data_details" ) ){
  
  data.details.file = paste0(
    dataset.directory , dhis_instance , "_details.rds"
    ) 
    
  data_details = readRDS( data.details.file ) 

  # get date
    data_details_date = file.info( data.details.file )$mtime
      
}
  
d = dataset.translated( data_details, .ous = ous , .meta = md  ) %>%
    left_join( ous.updated %>% 
                 dplyr::select( orgUnit , lat. , long. , impute ) %>%
                 rename( lat = lat. , long = long. ), 
               by = 'orgUnit'
               )  %>%
    # combine data element with category option combo
    unite( de.coc, dataElement, categoryOptionCombo, remove = FALSE ) %>%
    unite( de.coc.name , dataElement.name , categoryOptionCombo.name , remove = FALSE ) 

# glimpse(d)
# count( d, level )
# count( d, feature , level )

```


### orgunit with nested data 

```{r nest }


  nest_file = paste0( dataset.directory ,
                           dhis_instance, 
                           "_nest.rds" ) 

 
 if ( file.exists( nest_file)  ){
   
   nest_file_date = file.info( nest_file )$ctime 
   
 } else {
   
   nest_file_date = data_details_date 
}

# Update nest if existing nest file older than details file
  if (  params$create_new |  
        nest_file_date <= data_details_date 
        ) {
      
    # TODO: group_by depends on variables returned from ous.translated 
    d.nest = d %>% 
        complete( nesting( orgUnit, de.coc.name, period) ) %>%
        mutate( value = as.integer( value ) ) %>%
        group_by( level, level.name, feature
                  # , children
                  , orgUnit ,  orgUnit.name , lat, long , 
                  parent_ou.name ,  de.coc.name,  
                  dataElement, categoryOptionCombo,
                  dataElement.name , categoryOptionCombo.name 
                  
                  ) %>%
        nest( period, value ) 
    
    if ( file.exists( nest_file)  ){ 
        
      
      # Retrieve access data 
  
        file.rename( nest_file ,
                     paste0( nest_file , 
                             format( nest_file_date , "%b%d%Y")
                                                        )
                     )
    }
    
    saveRDS( d.nest , nest_file ) 
    
    nest_file_date = file.info( nest_file )$ctime 

  } else {
    
    if ( file.exists( nest_file)  ){
          
          d.nest = readRDS( nest_file )
      }
  }

     

```

### orgunit with nested time-series, and seasonal decomposition

```{r nest.ts }


  nest.ts_file = paste0( dataset.directory ,
                           dhis_instance, 
                           "_nest.ts.rds" ) 

 if ( file.exists( nest.ts_file)  ){
   
   nest_ts_file_date = file.info( nest.ts_file )$ctime 
   
 } else {
   
   nest_ts_file_date = data_details_date 
 }


   
  if ( params$create_new |
       nest_ts_file_date < nest_file_date
       ){ 
        
    
    print( "creating time-series for each orgUnit-data element")

    pb <- progress_estimated( nrow( d.nest ) )
    
    d.nest.ts = d.nest %>% 
        mutate( 
            ts = map( data,   ~ts.df(.x, .pb = pb)  ) ,
            n = map_dbl( ts, ~sum( !is.na(.x) ) ) ,
            total = map_dbl( ts , ~sum( .x, na.rm =  TRUE ) ) 
            ) 
    
    
    print( "performing decomposition for each orgUnit-data element")

    pb <- progress_estimated( nrow( d.nest ) )
    
      d.nest.ts = d.nest.ts %>% 
        mutate( 
            dec = map_dbl( ts, 
                       ~decompose.stl( .x , transform = "log", 
                                       plot = FALSE ,
                                       .pb = pb )  
                       )
            ) 
    
     if ( file.exists( nest.ts_file)  ){ 
        
        file.rename( nest.ts_file ,
                     paste0( nest.ts_file , 
                             format( nest_ts_file_date ,
                                     "%b%d%Y")
                                                        )
                     ) 
     }
 
    saveRDS( d.nest.ts ,  nest.ts_file  )

  } else {
    
    if ( file.exists( nest.ts_file) ){
          
          d.nest.ts = readRDS( nest.ts_file )
      }

}

```

### orgunit with nested seasonally decomposed time-series

In this dataset, there is a unique row for every OU and DE.  Attached to each row is a dataset corresponding to the OU and DE.The nested dataset contains a time-series of the number of reports (ts), the total number of reporting periods (n), the total number of reports( total), and a measure of the seasonally adjusted error (dec). 

```{r nest.dec , eval=FALSE}

  nest.dec_file = paste0( dataset.directory ,
                           dhis_instance, 
                           "_nest.dec.rds" ) 


     if ( file.exists( nest.dec_file) & !params$create_new ){
          
          d.nest.dec = readRDS( nest.dec_file )
      }

    if ( file.exists( nest.dec_file) & params$create_new  ){ 
        
      
      # Retrieve access data 
        file_date = file.info( nest.dec_file )$ctime 
  
        file.rename( nest.dec_file ,
                     paste0( nest.dec_file , 
                             format( file_date , "%b%d%Y")
                                                        )
                     )
    }

  if ( ( !exists( "d.nest.dec") | params$create_new ) ) {
      
    pb <- progress_estimated( nrow( d.nest ) )
    
    d.nest.dec = d.nest %>% 
        mutate( 
            ts = map( data,   ~ts.df(.x , .pb = pb )  ) ,
            n = map_dbl( ts, ~sum( !is.na(.x) ) ) ,
            total = map_dbl( ts , ~sum( .x, na.rm =  TRUE ) ) ,
            dec = map_dbl( ts, 
                       ~decompose.stl( .x , transform = "log", 
                                       plot = FALSE )  
                       )
            ) 
    
 
    saveRDS( d.nest.dec ,  nest.dec_file  )

}

```

## Data element with nested number of reports

unlike the nested datasets above, for which there is a unique row for every OU and DE, this dataset only has a unique row for every **DE**.  The nested data has the number of OU reporting each period. 



```{r}

library( sparkline )

de_reported_by_month = count( d.nest.dec %>% unnest( data ), dataElement.name  , de.coc.name, period )


pb <- progress_estimated( nrow( de_reported_by_month ) )
    
de.reported.ts = de_reported_by_month %>% 
       rename( value = nn ) %>%
       nest( period  , value ) %>%
        mutate( 
            ts = map( data,   ~ts.df(.x , .pb = pb )  ) ,
            n = map_dbl( ts, ~sum( !is.na(.x) ) ) ,
            total = map_dbl( ts , ~sum( .x, na.rm =  TRUE ) ) 
          , dec = map_dbl( ts,
                       ~decompose.stl( .x , transform = "log",
                                       plot = FALSE )
                       )
          
          , sparkline = map( ts, ~sparkline( .x %>% unlist ))
          # , n_recent = 
            ) 
 

sparkline( de.reported.ts[1, ]$ts %>% unlist )

 # View( de.reported.ts ) 
 
```

Chart of number of reports submitted with the key data elements

```{r}
library( sparkline )

d1 = data_element_table %>% 
  inner_join( de.reported.ts %>% 
                select(dataElement.name, de.coc.name , sparkline ) %>% mutate( sparkline = map( sparkline, 1 ) )
              , by = c('dataElement' = 'dataElement.name' ) 
              ) %>%
  datatable() 

d1$dependencies <- append(d1$dependencies,
                          htmlwidgets:::getDependency("sparkline") )



```


```{r, eval = FALSE }

de.reported.ts[1, ]$ts[[1]] %>% plot

library(ggfortify)
library( magrittr )

i = 7

plot.ts = function( i ){ 
    a = map( de.reported.ts$ts , autoplot  ) %>% extract(i)
    a[[1]] + labs( title = de.reported.ts[ i , "de.coc.name" ] , 
                   y = 'Reports', x = '' ,
                   # subtitle = de.reported.ts[ i , "total" ] %>% 
                   #     as.integer %>% scales::comma() ,
                   subtitle =   de.reported.ts[ i , "dec" ] %>% round( 2 ))
}

# plot.ts(23)

library( patchwork )
# plot.ts(24) + plot.ts(23) 
map( 1:5 , ~plot.ts( .x ) ) %>% reduce( `+` )

library( ggrepel )
# de_reported_by_month %>% 
#     mutate( date = as.yearmon( period, "%Y%m" ) %>% as.Date() ) %>%
#     ggplot( aes( x = date, y = nn , group = de.coc.name, color = de.coc.name ) ) +
#     geom_line() +
#     scale_x_date( limits = c(as.Date("2014-01-01") , as.Date("2030-01-01") ) ) +
#     geom_text_repel( data =  de_reported_by_month %>% 
#                          filter( period %in% "201810" ) %>%
#                           mutate( date = as.yearmon( period, "%Y%m" ) %>% as.Date() ) ,
#                      aes( label = de.coc.name ) ,
#                      nudge_x = 10000 , 
#                      segment.size  = 0.2,
#                      segment.color = "grey50",
#                      direction     = "y"
#                      ) +
#     labs(     y = 'Reports', x = '' ) + 
#     scale_color_discrete( guide = FALSE ) +
#     theme_minimal() 
    
```



## Impute 

```{r d.nest.impute}

# data frame of datasets and data elements
dsde = map_df( 1:length(md$dataSets$dataSetElements), 
               ~map_df( md$dataSets$dataSetElements[[.x]], 
                        ~as.matrix(.x) )) %>%
    
    rename( dataElement.id = dataElement , 
            dataSet.id = dataSet ,
            categoryCombo.id = categoryCombo
            ) %>%
    
    left_join( md$dataElements %>% select( id, name, zeroIsSignificant ) ,
               by = c('dataElement.id' = 'id' )) %>%
    
    rename( dataElement = name ) %>%
    
    left_join( md$dataSets %>% select( id, name ) ,
               by = c('dataSet.id' = 'id' )) %>%
    
    rename( dataSet = name ) %>%
    
    left_join( md$categoryCombos %>% select( id, name ) ,
               by = c('categoryCombo.id' = 'id' )) %>%
    
    rename( categoryCombo = name ) 

de.all = unique( d.nest$dataElement.name )

datasets = dsde %>%  filter( dataElement %in% de.all )  %>% pull( dataSet ) %>% unique

# report of which data elements can recorded as zero for each dataset (split) ####
element.form = dsde %>% 
    filter( dataElement %in% de.all ) %>%
    filter( dataSet %in% datasets[1:6]) %>%
    select( dataElement.id, dataElement, dataSet, zeroIsSignificant ) %>%
    arrange( dataElement ) %>%
    spread( dataSet , zeroIsSignificant  ) 

# element.form %>% View

element.form = dsde %>% 
    filter( dataElement %in% de.all ) %>%
    filter( dataSet %in% datasets[7:length(datasets)] ) %>%
    select( dataElement.id, dataElement, dataSet, zeroIsSignificant ) %>%
    arrange( dataElement ) %>%
    spread( dataSet , zeroIsSignificant  ) 

# data.frame of ous assigned to datasets ####

ou.dataset = # data.frame with each orgunit - dataset combination (long form)
    
    md$dataSets %>% 
    
    select( id, name, organisationUnits ) %>%
    
    rename( dataSet.id = id , dataset.name = name ) %>%
    
    unnest %>% as_tibble() %>%
    
    # add in date OU 'opened'
    left_join( md$organisationUnits %>% select( id , openingDate, closedDate ) , by = "id" ) %>%
    
    rename( organistionUnit = id ) 

# choose data element ####

de.selected = de.all[ 21 ] #"NMCP OPD Confirmed Malaria Cases Through RDT

# de.selected = de.all[17] # IDSR Malaria [zero is recorded]


#  Data from 1 data element ( may contain >1 combos )

x = d.nest %>% filter( dataElement.name %in% de.selected )


categoryOptionCombo.for.dataElement = x %>% 
    
    filter( dataElement.name %in% de.selected ) %>%
    
    select( categoryOptionCombo , categoryOptionCombo.name )     %>% 
    
    unique 

# categoryOptionCombo.for.dataElement # 1 row=1 for <5years (Malawi)

categoryOptionCombo.selected = categoryOptionCombo.for.dataElement %>% 
    filter( row_number() == 1 ) %>%
    .$categoryOptionCombo.name



```

###  x: Data from 1 data element ( may contain >1 combos )  

```{r}
#  Data from 1 data element ( may contain >1 combos )

x = d.nest %>% filter( dataElement.name %in% de.selected ,  
                       categoryOptionCombo.name  %in% categoryOptionCombo.selected ) 

```

### x.ts

```{r}

library(forecast)
library( ggfortify)
library(imputeTS)

# see https://cran.rstudio.com/web/packages/sweep/vignettes/SW01_Forecasting_Time_Series_Groups.html 

log_1 = function( x ){
  isZero = which( x == 0 )
  x[ isZero ] = 0.000001 
  x = log( x )
  return( x )
}

# helper function to mimic log but allows zero
ihs = function(x) log( x +sqrt( x**2 + 1 ) ) 
ihs. = function(y) ( exp( 2 * y) - 1 ) / (2 * exp(y) )


my.tsclean = function( .x  , log = FALSE, .pb= NULL ){
  
  # progress bar
    update_progress( .pb ) 
  
  if ( sum( !is.na(.x) ) > 1 ){
    
    if ( log ){ 
      
      .x. = tsclean( log_1( .x ) %>% exp)
      # forces values to be positive (log)
      
    } else {

      .x. = tsclean(  .x )
   } 
    

    return( .x. )
  } else .x  # return original data
}

# test
# .x = ts.df( x[ 1, ]$data[[1]])
# .x = ts.df( x[ 100, ]$data[[1]])
# my.tsclean( .x  )
# my.tsclean( .x  , 0)
# my.tsclean( .x  ,  "auto")


# more tests... (need to remove zeros with log_1 function to get it to work )
# ts.clean.list = list( nrow(x) )
# x = d.nest %>% filter( dataElement.name %in% de.selected ,  
#                        categoryOptionCombo.name  %in% categoryOptionCombo.selected ) 
# 
# for ( i in 1:nrow(x)){
#   ts.clean.list = my.tsclean( ts.df( x[ i, ]$data[[1]] ) , log = TRUE  )
# }


pb = progress_estimated( nrow( x ) )

x.ts = x[ , ] %>% 
       mutate( 
            ts = map( data,  ~ts.df(.x, .pb = pb)  ) ,
            n = map_dbl( ts, ~sum( !is.na(.x) ) ) ,
            total = map_dbl( ts , ~sum( .x, na.rm =  TRUE ) ) 
            ) %>%
       mutate( 
            ts.clean = map( ts , ~my.tsclean( .x ) ) 
            # , total.clean =  map_dbl( ts.clean , ~sum( .x, na.rm =  TRUE ) ) 
             , ts.clean.log = map( ts , ~my.tsclean( .x , log = TRUE ) )
         )


```

### Total values and number reported 

```{r}

# functions to convert NA to zero , then add
na_to_y <- function(x, y){
  x[is.na(x)] <- y
  x # you need to return the vector after replacement
}

add <- function( ts.list ){
  ts.list = map( ts.list , na_to_y, 0 ) 
  Reduce("+", ts.list ) 
}
 # test: add(  x.ts$ts  ) ; add(  x.ts$ts.clean   )


# Total among those with minimum no. months reporting
min.months.reported = 24

total = add(  x.ts %>% 
                filter( n >= min.months.reported ,
                        feature == 'Point' ) %>% 
                pull(ts)   
              ) 

# Total *after cleaning* among those with minimum no. months reporting
total.clean = add(   x.ts %>% 
                       filter( n >= min.months.reported ,
                        feature == 'Point' ) %>%  
                       pull( ts.clean )     )

# Chart total of data element by month, Raw and Cleaned

ts.gg = ggplot() + 
    geom_line( data = total %>% ts_to_df() , 
               aes( x = date , y=Y), color = 'black', size = 1) +
  
    geom_line( data = total.clean  %>% ts_to_df() ,
               aes( x = date ,y=Y),
               color = 'blue', size = 1 ) +
    
    labs( title = de.all[ 21 ], 
          subtitle = categoryOptionCombo.selected , 
          caption = paste( params$dhis_instance , 
                           "as reported on" , 
                           substr( data_details_date , 1, 10 ) 
          ) , 
          y = "Cases" , x = "Month") +
    scale_y_continuous( label = comma ) +
    theme_minimal()


ts.gg


# no. reports per month 

 de.reported.ts = function( dataset = x.ts 
                            # , data = quo(ts) 
                            , .min.months.reported = 1 ){
   dataset %>% 
   filter( n >= min.months.reported ,
           feature == 'Point' ) %>% 
   unnest( data ) %>% 
   select( orgUnit, de.coc.name, period , value) %>% 
     
   count( de.coc.name, period ) %>%
   rename( value = n ) %>%
       nest( period  , value ) %>%
        mutate( 
            ts = map( data,   ~ts.df(.x )  ) ,
            n = map_dbl( ts, ~sum( !is.na(.x) ) ) ,
            total = map_dbl( ts , ~sum( .x, na.rm =  TRUE ) ) 
            # , dec = map_dbl( ts, 
            #            ~decompose.stl( .x , transform = "log", 
            #                            plot = FALSE )  
            #            )
            ) 
 } 
 
 # de.reported.ts()

# Chart number of reports submiteed per month, filtered for 
 # minimum number of months reporting 

reported.total = add(   de.reported.ts( 
  .min.months.reported = 1
  ) %>% 
    pull( ts )     
  ) %>%  
  ts_to_df 

 max.ever.reported = x.ts %>% filter( feature == 'Point' ) %>% nrow
 
 max.reported.minimum.months = x.ts %>% 
   filter( feature == 'Point' ,
           n >= min.months.reported 
           ) %>% nrow
  

reported.gg = ggplot() + 
    # geom_line( data = df.total , aes( x = date , y=Y), color = 'black') +
    geom_line( data = reported.total , 
               aes( x = date , y = Y ), color = 'black', size = 1 ) +

    labs( title = 'Number facilities reporting each month' , 
          # subtitle = categoryOptionCombo.selected , 
          caption = paste( params$dhis_instance , 
                           "as reported on" , 
                           substr( data_details_date , 1, 10 ) 
          ) , 
          y = "No. Facilities Reporting" , x = "Month") +
    scale_y_continuous( label = comma ) +
    theme_minimal() + 
    expand_limits(y = c(0, 1.2 * max(reported.total$Y )) ) +
    
    annotate( 'segment' , x = min( reported.total$date ) ,
              xend = max( reported.total$date ) ,
              y = max.ever.reported , yend = max.ever.reported ,
              color = 'purple' ) +
   annotate( 'text' , x = min( reported.total$date ) ,
              y = max.ever.reported , hjust = 0 , vjust = -.2 ,
              label = 'Total number of facilities ever reporting',
              color = 'purple' ) +

  annotate( 'segment' , x = min( reported.total$date ) ,
              xend = max( reported.total$date ) ,
              y = max.reported.minimum.months ,
            yend = max.reported.minimum.months ,
              color = 'blue' ) +
   annotate( 'text' , x = min( reported.total$date ) ,
              y = max.reported.minimum.months , hjust = 0 ,
             vjust = -.5,
              label = 'Number facilities reporting at least 24 months',
              color = 'blue' ) 
  
   # annotate( 'text' , x = mean( reported.total$date ) , 
   #            y = 400 , hjust = .5 , 
   #            label = 'Number facilities reporting each month',
   #            color = 'black' )

reported.gg

## Combine totals with no. reports 
library( patchwork )
ts.gg + reported.gg + plot_layout(ncol = 1, heights = c(3, 3))






```

```{r other_ts_trend_plots, eval=FALSE }

# annualize 
ts.annual = function( ts , .fun = sum ){
  aggregate( ts , nfrequency = 1 , FUN = .fun ) %>%
                 data.frame( date=as.Date(time( . )) , 
                             Y= as.matrix( . ) )
}

ggplot() + 
    geom_col( data =  ts.annual( total ) , 
               aes( x = date , y=Y ), fill = 'black') +
    scale_y_continuous( label = comma ) +
    labs(title = "Annual Case Counts (actual)", x = "", y = "Cases" )

ggplot() + 
    geom_col( data =  ts.annual( total.clean ) , 
               aes( x = date , y=Y ), fill = 'blue')  +
    scale_y_continuous( label = comma ) +
    labs(title = "Annual Case Counts (estimated)", x = "", y = "Cases" )


# Rolling mean
library( roll )
ts.rolling = function( ts ){
    
     data.frame(
        date=as.Date(time( ts )), 
         Y = roll_mean( as.matrix( ts ), width = 12  )
        )
}

ggplot() + 
    geom_line( data =  ts.rolling( total ) , 
               aes( x = date , y = Y ), color = 'black') +
    geom_line( data = ts.rolling( total.clean ) , 
               aes( x = date , y=Y ), color = 'blue') 

# Examples
min.months.reported = 36
max.months.reported = 36 

x.ts.size = x.ts %>% 
  filter( n >= min.months.reported ,  n <= max.months.reported ) 

xts.clean.df <- data.frame(
  date = map( x.ts.size$ts.clean  , ~as.Date( time( .x ) ) ) %>% unlist ,   Y = map( x.ts.size$ts.clean  , ~as.matrix( .x ) ) %>% unlist , 
  n = cut( x.ts.size$n , 3 ) ,
  orgUnit = x.ts.size$orgUnit,
  data = 'clean'
  )

xts.df <- data.frame(
  date = map( x.ts.size$ts  , ~as.Date( time( .x ) ) ) %>% unlist ,   Y = map( x.ts.size$ts  , ~as.matrix( .x ) ) %>% unlist , 
  n = cut( x.ts.size$n , 3 ) ,
  orgUnit = x.ts.size$orgUnit ,
  data = 'raw'
  )

ggplot() + 
    geom_line( data = xts.clean.df , aes( x = date , y = Y,
                                          group = orgUnit , color = data)
               ) +
  geom_line( data = xts.df , aes( x = date , y = Y, 
                                          group = orgUnit , color = data)
               ) + 
  facet_grid( orgUnit ~ .)



```


```{r nest.dec_summary_by_element }

  nest.dec_file = paste0( dataset.directory ,
                           dhis_instance, 
                           "_nest.dec.rds" ) 

  d.nest.dec = readRDS( nest.dec_file )


  # Summary by element 
  d.nest.dec %>%
      group_by( de.coc.name , level.name ) %>%
      summarise(
          # n = n()  ,
          wt.mean = weighted.mean( dec , total , na.rm = TRUE )
      ) %>%
      spread( level.name, wt.mean ) %>% pander
  
```


```{r choose.dec.data.element }

# choose data element ####
de.coc.all = unique(d$de.coc.name)

de.coc.selected = de.coc.all[13] # "NMCP OPD Suspected Malaria Cases Tested For Malaria Through RDT_<5Yrs"


# model d.nest for selected element/catoegory: "HMIS Malaria – New Case (under 5)" ####
x.dec = d.nest.dec %>% filter( de.coc.name %in% de.coc.selected )

```

## Percent Increase Attributable to Added Facilities

```{r}

# function to take list of time-series and summarise them
# filters based on number of non-missing values
# returns a single time-series with same time-basis (e.g. monthly)

ts_sum = function( nest.ts , 
                   col ,
                   min.months.reported = 0 ,
                   max.months.reported = 60 
                   ){

    # functions to convert NA to zero , then add
    na_to_y <- function(x, y){
      x[is.na(x)] <- y
      x # you need to return the vector after replacement
    }
    
    add <- function( ts.list ){
      ts.list = map( ts.list , na_to_y, 0 ) 
      Reduce("+", ts.list ) 
    }
     # test: add(  x.ts$ts  ) ; add(  x.ts$ts.clean   )

    sum.ts = add(  nest.ts   %>% filter( 
      n >= min.months.reported , 
      n <=  max.months.reported ) %>% pull( !!col )   
      ) 
    
    return( sum.ts )
}

# test
# ts_sum( x.ts , quo(ts) )

# function to take list of time-series and COUNT number non-missing
# filters based on number of non-missing values
# returns a single time-series with same time-basis (e.g. monthly)

ts_count = function( nest.ts , 
                     col ,
                     min.months.reported = 0 ,
                     max.months.reported = 60 
                   ){

  one_if_not_na <- function( x ){
      x[!is.na(x)] <- 1
      x[is.na(x)] <- 0
    x 
  }

  num_nonNA <- function( ts.list ){
    ts.list = map( ts.list , one_if_not_na ) 
    Reduce("+", ts.list ) 
  }
    
  count.ts = num_nonNA(  nest.ts   %>% filter( 
      n >= min.months.reported , 
      n <=  max.months.reported ) %>% pull( !!col )   
      ) 
    
    return( count.ts )
}

# test
# ts_count( x.ts , quo(ts) )

ts.annual = function( ts , .fun = sum ){
      ts.agg = aggregate( ts , nfrequency = 1 , FUN = .fun ) 
      return(ts.agg)
}

# Test
# ts_count( x.ts , quo(ts) ) %>% ts.annual()

ts_to_df = function( ts ){
      df <- data.frame( 
        date=as.Date(time( ts )) , 
        Y= as.matrix( ts ) 
      )
      
      return( df )
}

# Test
# ts_count( x.ts , quo(ts) ) %>% ts.annual() %>% ts_to_df()

raw.count = ts_count( x.ts , quo(ts) ) %>% ts.annual() %>% ts_to_df() %>%
  mutate( data ='raw' )

min_non_miss = c( 0 , 12, 24, 36, 48 )
map( min_non_miss , ~ts_count( x.ts , quo(ts), .x ) %>% 
       ts.annual() %>% ts_to_df() %>% .$Y)


clean.count = ts_count( x.ts , quo(ts.clean) ) %>% 
  ts.annual() %>% ts_to_df() %>%
  mutate( data ='cleaned' )

raw.sum = ts_sum( x.ts , quo(ts) ) %>% ts.annual() %>% ts_to_df() %>%
  mutate( data ='raw' )

clean.sum = ts_sum( x.ts , quo(ts.clean) ) %>% 
  ts.annual() %>% ts_to_df() %>%
  mutate( data ='cleaned' )

raw.avg =  ( 
   ( ts_sum( x.ts , quo(ts) ) %>% ts.annual() ) / 
   ( ts_count( x.ts , quo(ts) ) %>% ts.annual() )
                          ) %>% 
ts_to_df() %>%
  mutate( data ='raw' )

clean.avg = ( 
   ( ts_sum( x.ts , quo(ts.clean) ) %>% ts.annual() ) / 
   ( ts_count( x.ts , quo(ts.clean) ) %>% ts.annual() )
                          ) %>% 
ts_to_df() %>%
  mutate( data ='cleaned' )

ggplot() + 
    geom_line( data = raw.avg  , aes( x = date , y = Y) , color = 'black') +
    geom_line( data = clean.avg , aes( x = date , y = Y), color = 'blue') 


```

## impact of cleaning by number of months with non-missing reports

```{r}
min_non_miss = c( 0 , 21, 24, 48 )
max_non_miss = c( min_non_miss[ 2:length( min_non_miss ) ] , Inf )

raw.sum = map2_df( min_non_miss , max_non_miss, ~ts_sum( x.ts , quo(ts), .x , .y ) %>% 
                 # ts.annual() %>% 
                   ts_to_df() %>%
                 mutate( data ='raw', min_n = .x %>% factor, min_y = .y %>% factor )
)

clean.sum = map2_df( min_non_miss , max_non_miss, ~ts_sum( x.ts , quo(ts.clean), .x , .y ) %>% 
                 # ts.annual() %>% 
                   ts_to_df() %>%
                 mutate( data ='clean', min_n = .x %>% factor, min_y = .y %>% factor )
)

clean.llog.sum = map2_df( min_non_miss , max_non_miss, 
                            ~ts_sum( x.ts , quo(ts.clean.log), .x , .y ) %>% 
                 # ts.annual() %>% 
                   ts_to_df() %>%
                 mutate( data ='clean.log', min_n = .x %>% factor, min_y = .y %>% factor )
)

ggplot() + 
    geom_line( data = raw.sum  , aes( x = date , y = Y, group = min_n, color = data) ) +
    geom_line( data = clean.sum  , aes( x = date , y = Y, group = min_n, color = data) ) +
    geom_line( data = clean.log.sum  , aes( x = date , y = Y, group = min_n, color = data) ) +
    facet_grid( min_n ~ . , scales = 'free' )


```


## MAPE of imputation

```{r}

 interp.mape = function ( .x, n = NULL , tests = 25  
                              #' The na.interp function imputes missing values
                              #' This function attempts to estimate the mean absolute percentage error (MAPE) 
                              #' by simulating a number of missing values and then calculating the MAPE of the 
                              #' replacement values with the original values          
 ) 
 {
   
     nonNA = which( !is.na( .x ) & ( .x > 0 ) %>% as.logical()  )
     
     if ( is.null( n ) ){
         n = round( length( nonNA )  / 5 , 0 )
     }
     
     mape = list( tests )
     
     for ( test in 1:tests ){ 
       
         simulate.missing.index = sample( nonNA , n ) %>% 
           as.integer() %>% unique() 
         
         simulate.missing.index = simulate.missing.index[ order(simulate.missing.index) ]
         
         x.sim = .x 
         
         x.sim[ simulate.missing.index ] =  NA
         
         replacements <- my.tsclean( x.sim , log = TRUE )
         
         differences = replacements[ simulate.missing.index ] - .x[ simulate.missing.index ]
         
         mape[ test ] =  sum( differences / .x[ simulate.missing.index ] ) / length( differences )
         
     }
     
     return( list( 
       mean = mean( unlist( mape ) , na.rm = TRUE  ) ,
       sd = sd( unlist( mape ) , na.rm = TRUE  ) 
       )
     )
 }


# Tests

interp.mape( x.ts[ which( x.ts$n > 0 &  x.ts$n < 12  )[1] , ]$ts[[1]] )

```


## Regional time-series

```{r}

count( x.ts, level, parent_ou.name )

regional.ts = x.ts %>% 
  filter( n >= 48 , !is.na( parent_ou.name ) , level == 4 
          ) %>%
  group_by( parent_ou.name ) %>%
  summarise( 
    n = n() 
    ) 

regional.ts$ts.clean = map( regional.ts$parent_ou.name , 
                      ~ts_sum( x.ts %>% filter( n >= 48 ,
                                                parent_ou.name == .x ),
                                                quo(ts.clean) ) 
                               ) 

# see all districts   
gg.regional.ts = 
  regional.ts %>% unnest %>% 
  bind_cols( map_df( regional.ts$ts.clean, ~ts_to_df(.x) )
             ) %>% 
  group_by( parent_ou.name ) %>%
  mutate( Y = scale( Y ) ) %>%
  ungroup %>%
  ggplot() +
  geom_line( aes( x=date, y= Y, group = parent_ou.name, color = n ))

plotly::ggplotly( gg.regional.ts )


# examine specific district
dataset =  x.ts %>% 
  filter( n >= 48 , parent_ou.name %in% "Nsanje-DHO" ) %>%
  select( orgUnit.name , parent_ou.name , n, ts.clean, ts )

gg.dho.ts = 
  dataset %>% unnest %>% 
  bind_cols( map_df( dataset$ts, ~ts_to_df(.x) )
             ) %>% 
  group_by( orgUnit.name ) %>%
  mutate( Y = scale( Y ) ) %>%
  ungroup %>%
  ggplot() +
  geom_line( aes( x=date, y= Y, group = orgUnit.name , color = n ))

plotly::ggplotly( gg.dho.ts )

```


## Aggregations

Table of District level Error

```{r}
tsd.parent = x.dec %>% 
  filter( feature %in% "Polygon" ) %>%
  dplyr::select( orgUnit.name , level , total, dec ) %>%
  mutate( bins = cut( dec , breaks = seq(0, 1, .1 ),  labels = seq(0.05, .95, .1 ) ) %>% 
              as.character() %>% as.numeric() )

kable( tsd.parent )
    
# hist( tsd.parent ) 
    
library( ggrepel ) 

quality = cut( tsd.parent$dec, breaks = c( 0, .25, .5 , 1, Inf) , 
                         labels = c('Best','Good', 'Sub-Standard', 'Poor') )

ggplot(  tsd.parent, aes(x = bins, y = 1, fill = quality) ) +
  geom_bar(stat = "identity", colour = "black", width = .1 , alpha = .5  ) +
  geom_text_repel( aes(label= ifelse( bins >= .5 , orgUnit.name, "") ),
             # position=position_stack(vjust=0.5), 
             colour="black",
             segment.size = 1 ,
             nudge_y = 2, 
             direction = 'y' ) +
theme_minimal() +
scale_fill_discrete( drop = FALSE ) +
labs( y = 'Count', x = 'Deviation' )


```


```{r}

x.dec %>% 
    filter( parent_ou.name %in% "Chiradzulu-DHO") %>%
    select( orgUnit.name, feature, level.name, total , dec ) %>%
    
  ggplot(  aes(x = bins, y = 1, fill = quality) ) +
  geom_bar(stat = "identity", colour = "black", width = .1 , alpha = .5  ) +
  geom_text_repel( aes( label= orgUnit.name ),
             # position=position_stack(vjust=0.5), 
             colour="black",
             segment.size = 1 ,
             nudge_y = 2, 
             direction = 'y' ) +
theme_minimal() +
scale_fill_discrete( drop = FALSE ) +
labs( y = 'Count', x = 'Deviation' )
    
```


```{r}

library(ggfortify)

tsd =  x.dec %>% 
  filter( feature %in% "Point" , 
          parent_ou.name %in% "Chitipa-DHO" 
          ) %>% 
  mutate( error = cut( dec, breaks = c(0, .25, .5, .75, 1, 1.25, 1.5, 1.75 , 2 , Inf ))
          )

summary( tsd$dec )

  g = ggplot( data = tsd , aes( x = error , y = 1 , label = orgUnit.name )  ) + 
    geom_col(  ) +
    scale_x_discrete(drop = FALSE) +
    theme_bw()
  
  plotly::ggplotly( g )

tsd.parent = x.dec %>% 
  filter( feature %in% "Polygon" , 
          orgUnit.name %in% "Chitipa-DHO" ) 
tsd.parent$dec


a = NULL
nf = nrow( tsd )
for ( i in 1:nf ){
  a = cbind( a, tsd$ts[[i]])  
}

autoplot( a , facets = FALSE , legend = FALSE, ts.size = 2 ) +
  labs( y = "Case Counts\n", x = "\nMonth") +
  theme_bw() +
  theme( legend.position = 'none' , panel.border=element_blank() ) +
  theme(axis.ticks=element_blank() ,
          axis.title=element_text(size = 16, face = "bold") ,
          axis.text=element_text(size = 14, face = "bold") 
          )

```

## Forecast 

```{r}
 library( vars )
library( forecast )

autoplot( tsd.parent$ts[[1]] , facets = FALSE , legend = FALSE, ts.size = 1 ) +
  labs( y = "Case Counts\n", x = "\nMonth") +
  theme_bw() +
  theme( legend.position = 'none' , panel.border=element_blank() ) +
  theme(axis.ticks=element_blank() ,
          axis.title=element_text(size = 16, face = "bold") ,
          axis.text=element_text(size = 14, face = "bold") 
          )

autoplot( d.forecast <- forecast(tsd.parent$ts[[1]] , h = 12, level = c(80) ),  ts.size = 1 ,
          predict.colour = 'blue', predict.linetype = 'dashed') +
  labs( y = "Case Counts\n", x = "\nMonth") +
  theme_bw() +
  theme( legend.position = 'none' , panel.border=element_blank() ) +         theme(axis.ticks=element_blank() ,
          axis.title=element_text(size = 16, face = "bold") ,
          axis.text=element_text(size = 14, face = "bold") 
          )


```

- higher quality examples

```{r}
tsd =  x.dec %>% 
  filter( feature %in% "Point" , 
          parent_ou.name %in% "Mulanje-DHO" ) 

# summary( tsd$dec )
ggplot( ) + geom_histogram( data = tsd , aes( x = dec )) 

tsd.parent = x.dec %>% 
  filter( feature %in% "Polygon" , 
          orgUnit.name %in% "Mulanje-DHO" ) 
tsd.parent$dec


a = NULL
nf = nrow( tsd )
for ( i in 1:nf ){
  a = cbind( a, tsd$ts[[i]])  
}

autoplot( a , facets = FALSE , legend = FALSE, ts.size = 1 ) +
  theme_bw() +
  theme( legend.position = 'none' , panel.border=element_blank() ) +
  theme(axis.ticks=element_blank() ,
          axis.title=element_text(size = 16, face = "bold") ,
          axis.text=element_text(size = 14, face = "bold") 
          ) 

autoplot( tsd.parent$ts[[1]] , facets = FALSE , legend = FALSE, ts.size = 1) +
  theme_bw() +
  theme( legend.position = 'none' , panel.border=element_blank() ) +
  theme(axis.ticks=element_blank() ,
          axis.title=element_text(size = 16, face = "bold") ,
          axis.text=element_text(size = 14, face = "bold") 
          ) 


autoplot( d.forecast <- forecast(tsd.parent$ts[[1]] , h = 12, level = c(95) ),  ts.size = 1 ,
          predict.colour = 'blue', predict.linetype = 'dashed') +
  labs( y = "Case Counts\n", x = "\nMonth") +
  theme_bw() +
  theme( legend.position = 'none' , panel.border=element_blank() ) +         theme(axis.ticks=element_blank() ,
          axis.title=element_text(size = 16, face = "bold") ,
          axis.text=element_text(size = 14, face = "bold") 
          )

 
```

# Data quality histograms for each data element

```{r x.dec_histogram }

instance = params$dhis_instance

quality_histogram = function( element = NULL , 
                              d.nest.dec. = d.nest.dec ,
                              subtitle = instance ,
                              .pb = NULL ){
    
    # progress bar
    update_progress( .pb ) 
    
    x.dec = d.nest.dec. %>% filter( de.coc.name %in% element )

    x.dec$quality = cut( x.dec$dec, breaks = c( 0, .5, 1 , 2, Inf) , labels = letters[1:4] )
    
    x.dec = x.dec %>% arrange( level )
    
    # Change names for Kenya.demo
    if ( params$dhis_instance %in% 'Kenya.demo' ){
        
        kenya.labels = c( 'National' , 'Region', "District", "Sub-District" , "Ward" , "Facility", "Community" )
        
        kenya.levels = c( 'National' , 'SNU1', "SNU2", "SNU3" , "SNU4" , "Facility", "Community" )
    
    
    x.dec$level.name = factor( x.dec$level.name, 
                               levels = kenya.levels ,
                               labels = kenya.labels ,
                               ordered = TRUE )
    
    
    }
    
    hist = x.dec %>% ggplot() + 
            geom_bar( aes(quality)  ) + 
            scale_x_discrete( 'Quality' , drop = FALSE ) +
            facet_wrap(~level.name, scales = 'free' ) +
            labs( title = element , subtitle = subtitle  )
    
    print( hist )
}


n_elements = length( de.coc.all )

pb = progress_estimated(n_elements)

# h = map( 1:n_elements  , ~quality_histogram( de.coc.all[.x] , .pb = pb) )
# 
# map( 1:n_elements , ~h[[.x]] )
# 
# walk( 1:n_elements , ~modify_depth( h[[.x]] , .depth = 0 , print ) )

walk( 1:n_elements , ~quality_histogram( de.coc.all[.x] , .pb = pb) )

```


# Quality Leaflet

```{r map.dec }

radius_factor = 1e4

quality_leaflet = function( element = NULL , 
                            d.nest.dec. = d.nest.dec ,
                            subtitle = instance ,
                            .pb = NULL ,
                            radius_factor = 1e4 ,
                            title.on.map = FALSE 
                            ){
    
    x.dec = d.nest.dec. %>% filter( de.coc.name %in% element )
    
    x.dec$quality = cut( x.dec$dec, 
                         breaks = c( 0, .5, 1 , 2, Inf) , 
                         labels = letters[1:4] )
    
    
    region.dec = x.dec %>% filter( level == 2 , feature %in% 'Polygon' )
    
    regions = admins[ match( region.dec$orgUnit, admins$id ), ]$polygons 
    
    map.region = SpatialPolygonsDataFrame( regions , region.dec,
                                               match.ID = FALSE )
        
    district.dec = x.dec %>% filter( level == 3 , feature %in% 'Polygon' ) 
    
    if ( all( is.na( district.dec$dec )) ) return()
        
    districts = admins[ match( district.dec$orgUnit, admins$id ),
                        ]$polygons 
    
    
    map.district = SpatialPolygonsDataFrame( districts , district.dec,
                                             match.ID = FALSE )
    
        
    
    x.facilities = x.dec %>% filter( feature %in% 'Point' )
    
    if ( all( is.na( x.facilities$quality ) ) ) return()
        
    
    binpal <- colorBin( brewer.pal( 5, "Reds"), 
                        domain = map.district$dec , bins =5, 
                        na.color = "#bdbdbd", pretty = TRUE
        )
        
    factpal <- colorFactor( 
            c('dark red','orange','yellow','dark green'), 
            x.facilities$quality , reverse = TRUE )
        
    
    radius_factor = mean( x.facilities$total , na.rm = T) * 3
        
        # Find a center point for each region
        centers.region <- data.frame(gCentroid(map.region, byid = TRUE))
        centers.region$name <- map.region$orgUnit.name
        centers.region$dec <- map.region$dec
        
        centers.district <- data.frame(gCentroid(map.district, byid = TRUE))
        centers.district$name <- map.district$orgUnit.name
        centers.district$dec <- map.district$dec
        
        # title
        tag.map.title <- tags$style(HTML("
          .leaflet-control.map-title { 
            transform: translate(-50%,20%);
            position: fixed !important;
            left: 50%;
            text-align: center;
            padding-left: 10px; 
            padding-right: 10px; 
            background: rgba(255,255,255,0.5);
            font-weight: bold;
            font-size: 28px;
            width: 100%;
          }
        "))

        title <- tags$div(
          tag.map.title, HTML( element )
        )  
        
    regionMap = leaflet(width=900, height=650) %>%
              
          # base map
          # addProviderTiles("Hydda.Base") %>%
      
          addTiles(  urlTemplate =
                           "http://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png"
            )  %>%
            
        addPolygons( data = map.region ,
                         group = 'Region' ,
                         color = "black",
                         weight = 1 ,
                         opacity = 1 ,
                         # label = ~paste( scales::percent(dec) ),
                         # labelOptions = labelOptions(noHide = TRUE, textOnly = TRUE, textsize = "14px") ,
                         popup = ~paste( orgUnit.name , percent( dec ) )  ,
                         fillColor =  ~binpal(dec),
                         fillOpacity = .5
            ) %>%
        addPolygons( data = map.district ,
                         group = 'District' , 
                         color = "black", 
                         weight = 1 , 
                         opacity = 1 ,
                         # label = ~paste( scales::percent(dec) ),
                         # labelOptions = labelOptions(noHide = TRUE, textOnly = TRUE, textsize = "14px") ,
                         popup = ~paste( orgUnit.name  , percent( dec ) )  ,
                         fillColor =  ~binpal(dec),
                         fillOpacity = .5
            ) %>%
            
        addCircleMarkers( data =  x.facilities , 
                              ~long , ~lat , 
                              radius = ~total/ radius_factor  , 
                              fillColor = ~factpal( quality ) ,
                              fillOpacity = 1 , 
                              weight = 1 ,
                              group = 'Facilities' ,
                              color = 'black' ,
                              opacity = .5 ,
                              popup = ~paste( orgUnit.name, "total:" , comma(total) ,
                                              "quality:" , percent( dec ) ) 
                              ) %>%
            
        addLabelOnlyMarkers(data = centers.district,
                                # group = 'District' ,
                                lng = ~x, lat = ~y, label = ~dec,
                                labelOptions = labelOptions(noHide = F, textOnly = TRUE, textsize = "15px" )
                                
                                ) %>% 
            
        addLegend(position = "bottomright", pal = binpal, 
                      values = map.district$dec,
                      title = "Quality",
                      opacity = 1 ) %>%
            
        addLegend(position = "bottomright", group = "Facilities" , 
                      pal = factpal , values = x.facilities$quality,
                      title = "Facilties",
                      opacity = 1 ) %>%
         
            # Layers control
        addLayersControl(
                overlayGroups = c("Region", "District", 'Facilities'), 
                # options = layersControlOptions(collapsed = FALSE),
                position = "topright"
            ) 
        
        if ( title.on.map ){  
            
            regionMap = regionMap %>% addControl( title, 
                                         position = "topleft",
                                         className="map-title" 
                                         )
        }
                         
        print( regionMap )
}


n_elements = length( de.coc.all )

# pb = progress_estimated(n_elements)

# l = map( 1:n_elements  , ~quality_leaflet( de.coc.all[.x] , .pb = pb) )

# walk( 1:n_elements , ~quality_leaflet( de.coc.all[.x] , .pb = pb) )

```


```{r , results='as.is' }

n_elements = length( de.coc.all )

html <- list()

i = 16  ; de.coc.all[16] # "NMCP OPD Confirmed Malaria Cases Through RDT_<5Yrs"

 html <- c( html, 
             list( h3( paste0( de.coc.all[i] 
                               , "(", i, ")" 
                       ) ),
                 
                 quality_leaflet( de.coc.all[i] , title.on.map = TRUE )
                 
                 )
  )
 
 tagList(html)
 
 # Create for all elements ...
 html <- list()
 for (i in 1:n_elements) {
    
  html <- c( html, 
             list( h3( paste0( de.coc.all[i] 
                               , "(", i, ")" 
                       ) ),
                 
                 quality_leaflet( de.coc.all[i] , title.on.map = TRUE )
                 
                 )
  )
                 
}

 tagList(html)

# saveRDS( html, paste("leaflet", i , ".html" ) )

```


# Quality GGPLOT

```{r ggplot_map }



quality_ggMap = function( element = NULL , 
                            d.nest.dec. = d.nest.dec ,
                            subtitle = instance ,
                            .pb = NULL,
                            radius_factor = 1e2 
                          ){
    
    
    x.dec = d.nest.dec. %>% filter( de.coc.name %in% element )
    
    x.dec$quality = cut( x.dec$dec, breaks = c( 0, .5, 1 , 2, Inf) , 
                         labels = c('Best','Good', 'Sub-Standard', 'Poor') )
    
    x.facilities = x.dec %>% filter( feature %in% 'Point' )
    
    region.index = which( x.dec$level == 2 & 
                              x.dec$feature %in% 'Polygon'   )
    
    region.dec = x.dec[ region.index ,] %>% 
        select( level, feature, orgUnit, orgUnit.name,
                de.coc.name, n, total, dec, quality )
    
    # get admin polygons
    regions.polys = admins[ match( region.dec$orgUnit, admins$id ),
                            ]$polygons 
    
    # assign orgunit id to each polygon
    region.id = region.dec$orgUnit 
    for (i in 1:length(region.id) ){
      slot(slot( regions.polys, "polygons")[[i]], "ID") = region.id[i]
    }
    
    # convert from SP polygons to list of coordinates; data is not carried over
    regions.g = regions.polys %>% broom::tidy(.) %>% rename( orgUnit = id )
    
    # Add data back to coordinates
    regions.g.df = left_join( regions.g, region.dec, by = "orgUnit" )
    
    # glimpse(regions.g)
    
    district.dec = x.dec %>% filter( level == 3 , feature %in% 'Polygon' ) 
    districts = admins[ match( district.dec$orgUnit, admins$id ),
                        ]$polygons 
    map.district = SpatialPolygonsDataFrame( districts , district.dec,
                                             match.ID = FALSE )
    
    map.g = ggplot( regions.g.df ) + 
      aes( long, lat ) + 
      geom_polygon( aes( group = group 
                         , fill = cut( dec,
                         breaks = c(0,.15,.25, .5, Inf))
                         # , fill = dec
                         ) , alpha = .2  
                    ) +
      geom_path(color="white") +
      geom_point( data = x.facilities ,
                  aes( color = quality , size = total  )
                  ) +
      coord_equal() +
      theme_map() +
      theme( legend.justification = c(1, 0), legend.position = 'right' ) +
      # scale_fill_brewer( "Region", drop = FALSE, palette = 'Reds' , direction = -1 ) +
      scale_fill_brewer( "Facility", drop = FALSE, type = 'div'  , direction = -1 , guide = FALSE ) +
      scale_color_brewer( "Facility", drop = FALSE, type = 'div'  , direction = -1 ) +
      scale_size_continuous( range = c( 1, 4 ) ) +
      labs( subtitle = subtitle,  title = element )
    
    print( map.g )
    
}

# TEST:
    quality_ggMap( element = de.coc.all[16] ) 
    
    x.dec = d.nest.dec. %>% filter( de.coc.name %in% element )
    
    x.dec$quality = cut( x.dec$dec, breaks = c( 0, .5, 1 , 2, Inf) , 
                         labels = c('Best','Good', 'Sub-Standard', 'Poor') )
    
    ggplot( count( x.dec , quality ) ) + geom_col( aes( x = quality , y = nn  ) ) +
        labs( title = 'Count of facilities', y = "", x = 'Quality' )
    
```

```{r print_ggplots }


quality_ggMap( element = de.coc.all[16] ) 

n_elements = length( de.coc.all )

pb = progress_estimated(n_elements)

# h = map( 1:n_elements  , ~quality_histogram( de.coc.all[.x] , .pb = pb) )
# 
# map( 1:n_elements , ~h[[.x]] )
# 
# walk( 1:n_elements , ~modify_depth( h[[.x]] , .depth = 0 , print ) )

walk(  1:n_elements , ~quality_ggMap( de.coc.all[.x] , .pb = pb) )

```




